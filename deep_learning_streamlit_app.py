
import streamlit as st
import inspect
import types
import json
import io

# Try to import PIL for image display if available
try:
    from PIL import Image
    PIL_AVAILABLE = True
except Exception:
    PIL_AVAILABLE = False

st.set_page_config(page_title="Deep Learning Models – Streamlit", layout="wide")

st.title("Deep Learning Models – Interactive App")
st.caption("This app was auto-generated from **Deep_learning_models.ipynb**. "
           "It loads the notebook's code and lets you call its functions interactively.")

@st.cache_resource(show_spinner=False)
def load_env():
    # Execute the notebook code in an isolated namespace
    env = {}
    code = r"#!/usr/bin/env python3\n\"\"\"\nDeep Learning Models for Customer Satisfaction Prediction\nImplements and compares 6 different deep learning architectures\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score\nfrom sklearn.metrics import precision_recall_fscore_support, recall_score, precision_score\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model, Sequential\n\nfrom nltk.corpus import wordnet\nfrom tensorflow.keras.layers import (\n    Dense, LSTM, Bidirectional, Conv1D, MaxPooling1D, GlobalMaxPooling1D,\n    Embedding, Dropout, Input, concatenate, Attention, MultiHeadAttention,\n    LayerNormalization, Add, GlobalAveragePooling1D, BatchNormalization, SpatialDropout1D\n)\n\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom tensorflow.keras.callbacks import (\n    EarlyStopping,\n    ReduceLROnPlateau,\n    TerminateOnNaN,\n    ModelCheckpoint,\n    CSVLogger\n)\n\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.utils import to_categorical\nfrom imblearn.over_sampling import SMOTE\nimport pickle\nimport warnings\nimport os\nimport json\nfrom datetime import datetime\nwarnings.filterwarnings('ignore')\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Load preprocessed data\nprint(\"Loading preprocessed data...\")\ndata = np.load('data/preprocessed_data.npz')\n\nX_num_train = data['X_num_train']\nX_num_val = data['X_num_val']\nX_num_test = data['X_num_test']\nX_text_train = data['X_text_train']\nX_text_val = data['X_text_val']\nX_text_test = data['X_text_test']\nX_num_train_balanced = data['X_num_train_balanced']\nX_text_train_balanced = data['X_text_train_balanced']\ny_train = data['y_train']\ny_val = data['y_val']\ny_test = data['y_test']\ny_train_balanced = data['y_train_balanced']\n\n# Load TF-IDF based data\nX_train_mlp = data['X_train_mlp']\nX_val_mlp = data['X_val_mlp']\nX_test_mlp = data['X_test_mlp']\ny_train_mlp = data['y_train_mlp']\ny_val_mlp = data['y_val_mlp']\ny_test_mlp = data['y_test_mlp']\n\n# Load metadata\nwith open('data/metadata.pkl', 'rb') as f:\n    metadata = pickle.load(f)\n\nvocab_size = metadata['vocab_size']\nmax_len = metadata['max_sequence_length']\nnum_features = len(metadata['feature_columns'])\nnum_classes = metadata['num_classes']\nclass_weights = metadata['class_weights']\nclass_names = metadata['class_names']\n\nprint(f\"Vocab size: {vocab_size}\")\nprint(f\"Max sequence length: {max_len}\")\nprint(f\"Number of features: {num_features}\")\nprint(f\"Number of classes: {num_classes}\")\nprint(f\"Training with balanced data: {X_text_train_balanced.shape[0]} samples\")\n\n\n\nclass DeepLearningModels:\n    def __init__(self, vocab_size, max_len, num_features, num_classes, embedding_dim=128):\n        self.vocab_size = vocab_size\n        self.max_len = max_len\n        self.num_features = num_features\n        self.num_classes = num_classes\n        self.embedding_dim = embedding_dim\n        self.models = {}\n        self.histories = {}\n\n    def create_mlp_model(self, input_dim):\n        \"\"\"Model 6: Deep MLP with TF-IDF features\"\"\"\n        model = Sequential([ Input(shape=(input_dim,)),\n                  # First hidden layer with batch normalization\n                  Dense(1024, activation='relu', kernel_regularizer=l2(0.001), input_shape=(input_dim,)),\n                  BatchNormalization(),\n                  Dropout(0.6),\n                  # Second hidden layer\n                  Dense(512, activation='relu', kernel_regularizer=l2(0.001)),\n                  BatchNormalization(),\n                  Dropout(0.5),\n                  # Third hidden layer\n                  Dense(256, activation='relu'),\n                  Dropout(0.4),\n                  Dense(128, activation='relu'),\n                  # Output layer\n                  Dense(self.num_classes, activation='softmax')\n        ])\n        return model\n\n\n    def create_lstm_model(self):\n        \"\"\"Model 1: LSTM-based RNN for sequential text processing\"\"\"\n        # Text input branch\n        text_input = Input(shape=(self.max_len,))\n        x = Embedding(self.vocab_size, self.embedding_dim, mask_zero=True)(text_input)\n        x = SpatialDropout1D(0.3)(x)\n        x = LSTM(128, return_sequences=True)(x)\n        x = LSTM(64)(x)\n\n        # Numerical input branch\n        num_input = Input(shape=(self.num_features,))\n        y = Dense(64, activation='relu')(num_input)\n        y = BatchNormalization()(y)\n\n        # Combine branches\n        z = concatenate([x, y])\n        z = Dense(128, activation='relu')(z)\n        z = Dropout(0.5)(z)\n        output = Dense(self.num_classes, activation='softmax')(z)\n\n        model = Model(inputs=[text_input, num_input], outputs=output)\n        optimizer = Adam(learning_rate=0.001, clipvalue=0.5)\n        model.compile(optimizer=optimizer,\n                      loss='sparse_categorical_crossentropy',\n                      metrics=['accuracy'])\n        return model\n\n    def create_bilstm_attention_model(self):\n        \"\"\"Model 2: Bidirectional LSTM with attention mechanism\"\"\"\n        # Text input branch\n        text_input = Input(shape=(self.max_len,))\n        x = Embedding(self.vocab_size, self.embedding_dim, mask_zero=True)(text_input)\n        x = Bidirectional(LSTM(128, return_sequences=True))(x)\n\n        # Attention\n        attention = MultiHeadAttention(num_heads=4, key_dim=64)(x, x)\n        x = LayerNormalization()(x + attention)\n\n        # Numerical branch\n        num_input = Input(shape=(self.num_features,))\n        y = Dense(64, activation='swish')(num_input)\n        y = LayerNormalization()(y)\n\n        # Combined\n        context = GlobalAveragePooling1D()(x)\n        z = concatenate([context, y])\n        z = Dense(256, activation='swish')(z)\n        z = Dropout(0.4)(z)\n        output = Dense(self.num_classes, activation='softmax')(z)\n\n        model = Model(inputs=[text_input, num_input], outputs=output)\n        model.compile(optimizer=Adam(learning_rate=0.0005),\n                      loss='sparse_categorical_crossentropy',\n                      metrics=['accuracy'])\n        return model\n\n    def create_robust_model(self):\n        # Text\n        text_input = Input(shape=(self.max_len,))\n        x = Embedding(self.vocab_size, 128)(text_input)\n        x = Bidirectional(LSTM(64))(x)\n\n        # Numerical\n        num_input = Input(shape=(self.num_features,))\n        y = Dense(64)(num_input)\n\n        # Combined\n        z = concatenate([x, y])\n        z = Dense(128, activation='relu')(z)\n        output = Dense(self.num_classes, activation='softmax')(z)\n\n        model = Model(inputs=[text_input, num_input], outputs=output)\n        model.compile(\n            optimizer='adam',\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        return model\n\n    def create_accurate_bilstm_attention_model(self):\n        \"\"\"High-performance BiLSTM with Attention\"\"\"\n        # Text input branch\n        text_input = Input(shape=(self.max_len,), name='text_input')\n        text_embedding = Embedding(\n            self.vocab_size,\n            self.embedding_dim * 2,  # Increased capacity\n            mask_zero=True\n        )(text_input)\n\n        # Enhanced Bidirectional LSTM\n        bilstm = Bidirectional(\n            LSTM(128,  # Doubled units\n                 dropout=0.3,\n                 recurrent_dropout=0.25,\n                 return_sequences=True,\n                 kernel_regularizer=l2(1e-4))  # Added regularization\n        )(text_embedding)\n        bilstm = BatchNormalization()(bilstm)  # Stabilizes training\n\n        # Powerful attention mechanism\n        attention = MultiHeadAttention(\n            num_heads=8,  # More attention heads\n            key_dim=128,  # Matches LSTM units\n            dropout=0.2,\n            kernel_regularizer=l2(1e-4)\n        )(bilstm, bilstm)\n\n        # Residual connection with layer norm\n        attention = Add()([bilstm, attention])\n        attention = LayerNormalization()(attention)\n\n        # Context extraction\n        text_features = GlobalAveragePooling1D()(attention)\n\n        # Enhanced numerical branch\n        num_input = Input(shape=(self.num_features,), name='numerical_input')\n        num_dense = Dense(64, activation='relu', kernel_regularizer=l2(1e-4))(num_input)\n        num_dense = BatchNormalization()(num_dense)\n\n        # Feature fusion\n        combined = concatenate([text_features, num_dense])\n        combined = Dropout(0.4)(combined)\n\n        # Deep classifier head\n        hidden = Dense(256, activation='relu', kernel_regularizer=l2(1e-4))(combined)\n        hidden = BatchNormalization()(hidden)\n        hidden = Dropout(0.4)(hidden)\n        hidden = Dense(128, activation='relu')(hidden)\n        output = Dense(self.num_classes, activation='softmax')(hidden)\n\n        model = Model(inputs=[text_input, num_input], outputs=output)\n        return model\n\n    def create_cnn_model(self):\n        \"\"\"Model 3: CNN for text classification with multiple filter sizes\"\"\"\n        # Text input branch\n        text_input = Input(shape=(self.max_len,))\n        x = Embedding(self.vocab_size, 128)(text_input)\n\n        # Parallel conv branches with residual connections\n        convs = []\n        for filter_size in [3, 5, 7]:\n            conv = Conv1D(128, filter_size, padding='same', activation='relu')(x)\n            conv = MaxPooling1D(2)(conv)\n            conv = Conv1D(64, filter_size, padding='same', activation='relu')(conv)\n            conv = GlobalMaxPooling1D()(conv)\n            convs.append(conv)\n\n        x = concatenate(convs) if len(convs) > 1 else convs[0]\n\n        # Numerical branch\n        num_input = Input(shape=(self.num_features,))\n        y = Dense(64, activation='relu')(num_input)\n        y = BatchNormalization()(y)\n\n        # Combined\n        z = concatenate([x, y])\n        z = Dense(128, activation='relu')(z)\n        z = Dropout(0.5)(z)\n        output = Dense(self.num_classes, activation='softmax')(z)\n\n        model = Model(inputs=[text_input, num_input], outputs=output)\n        model.compile(optimizer=RMSprop(learning_rate=0.0005),\n                      loss='sparse_categorical_crossentropy',\n                      metrics=['accuracy'])\n        return model\n\n    def create_transformer_model(self):\n        \"\"\"Model 4: Transformer-based model (simplified BERT-like architecture)\"\"\"\n        # Text input branch\n        text_input = Input(shape=(self.max_len,))\n        x = Embedding(self.vocab_size, 128)(text_input)\n\n        # Positional encoding\n        positions = tf.range(start=0, limit=self.max_len, delta=1)\n        positions = Embedding(self.max_len, 128)(positions)\n        x = x + positions\n\n        # Transformer blocks\n        for _ in range(3):  # Additional layer\n            attn = MultiHeadAttention(num_heads=4, key_dim=32)(x, x)\n            x = LayerNormalization()(x + attn)\n            ffn = Dense(512, activation='gelu')(x)\n            ffn = Dense(128)(ffn)\n            x = LayerNormalization()(x + ffn)\n\n        x = GlobalAveragePooling1D()(x)\n\n        # Numerical branch\n        num_input = Input(shape=(self.num_features,))\n        y = Dense(64, activation='relu')(num_input)\n        y = LayerNormalization()(y)\n\n        # Combined\n        z = concatenate([x, y])\n        z = Dense(256, activation='gelu')(z)\n        z = Dropout(0.4)(z)\n        output = Dense(self.num_classes, activation='softmax')(z)\n\n        model = Model(inputs=[text_input, num_input], outputs=output)\n        model.compile(optimizer=Adam(learning_rate=0.0001),\n                      loss='sparse_categorical_crossentropy',\n                      metrics=['accuracy'])\n        return model\n\n    def create_hybrid_cnn_lstm_model(self):\n        \"\"\"Model 5: Hybrid CNN-LSTM model\"\"\"\n        # Text input branch\n        text_input = Input(shape=(self.max_len,))\n        x = Embedding(self.vocab_size, 128)(text_input)\n\n        # CNN part\n        conv1 = Conv1D(128, 3, padding='same', activation='relu')(x)\n        conv1 = MaxPooling1D(2)(conv1)\n        conv2 = Conv1D(128, 5, padding='same', activation='relu')(x)\n        conv2 = MaxPooling1D(2)(conv2)\n        x = concatenate([conv1, conv2])\n        x = BatchNormalization()(x)\n\n        # LSTM part\n        x = Bidirectional(LSTM(128))(x)\n\n        # Numerical branch\n        num_input = Input(shape=(self.num_features,))\n        y = Dense(64, activation='relu')(num_input)\n        y = BatchNormalization()(y)\n\n        # Combined\n        z = concatenate([x, y])\n        z = Dense(256, activation='relu')(z)\n        z = Dropout(0.5)(z)\n        output = Dense(self.num_classes, activation='softmax')(z)\n\n        model = Model(inputs=[text_input, num_input], outputs=output)\n        model.compile(optimizer=Adam(learning_rate=0.0005),\n                      loss='sparse_categorical_crossentropy',\n                      metrics=['accuracy'])\n        return model\n\n    def create_hybrid_cnn_lstm_model_modified(self):\n        \"\"\"Fixed version with all required imports\"\"\"\n        # Text input branch\n        text_input = Input(shape=(self.max_len,), name='text_input')\n        text_embedding = Embedding(self.vocab_size, self.embedding_dim)(text_input)\n\n        # CNN with MaxPooling\n        conv1 = Conv1D(128, 3, activation='relu', padding='same')(text_embedding)\n        conv1 = MaxPooling1D(2)(conv1)\n        conv2 = Conv1D(128, 5, activation='relu', padding='same')(text_embedding)\n        conv2 = MaxPooling1D(2)(conv2)\n\n        conv_combined = concatenate([conv1, conv2])\n        conv_combined = BatchNormalization()(conv_combined)  # Now properly imported\n\n        # LSTM\n        lstm_out = LSTM(128, dropout=0.2, recurrent_dropout=0.2)(conv_combined)\n\n        # Numerical branch\n        num_input = Input(shape=(self.num_features,), name='numerical_input')\n        num_dense = Dense(32, activation='relu')(num_input)\n\n        # Combine branches\n        combined = concatenate([lstm_out, num_dense])\n        hidden = Dense(128, activation='relu')(combined)\n        output = Dense(self.num_classes, activation='softmax')(hidden)\n\n        model = Model(inputs=[text_input, num_input], outputs=output)\n        return model\n\n    def compile_model(self, model, learning_rate=0.001):\n        \"\"\"Compile model with appropriate optimizer and loss function\"\"\"\n        model.compile(\n            optimizer=Adam(learning_rate=learning_rate),\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        return model\n\n    def create_callbacks(self):\n        \"\"\"Create training callbacks\"\"\"\n        early_stopping = EarlyStopping(\n            monitor='val_loss',\n            patience=15,\n            restore_best_weights=True\n        )\n\n        reduce_lr = ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=7,\n            min_lr=1e-7\n        )\n\n        return [early_stopping, reduce_lr]\n\n    def train_model_mlp(self, model, model_name, X_train, X_val, y_train, y_val,\n                    class_weights=None, epochs=100, batch_size=128):\n        \"\"\"Train a model with given data\"\"\"\n        print(f\"\\nTraining {model_name}...\")\n\n        #callbacks = self.create_callbacks()\n        callbacks = [\n            EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-5),\n            TerminateOnNaN()\n        ]\n\n        history = model.fit(\n            X_train, y_train,\n            validation_data=(X_val, y_val),\n            epochs=epochs,\n            batch_size=batch_size,\n            class_weight=class_weights,\n            callbacks=callbacks,\n            verbose=1\n        )\n\n        self.models[model_name] = model\n        self.histories[model_name] = history\n\n        return model, history\n\n    def train_model(self, model, model_name, X_text_train, X_num_train, y_train,\n                    X_text_val, X_num_val, y_val, class_weights, epochs=100):\n        \"\"\"Train a model with given data\"\"\"\n        print(f\"\\nTraining {model_name}...\")\n\n        callbacks = self.create_callbacks()\n\n        history = model.fit(\n            [X_text_train, X_num_train], y_train,\n            validation_data=([X_text_val, X_num_val], y_val),\n            epochs=epochs,\n            batch_size=16,\n            class_weight=class_weights,\n            callbacks=callbacks,\n            verbose=1\n        )\n\n        self.models[model_name] = model\n        self.histories[model_name] = history\n\n        return model, history\n\n    def evaluate_model(self, model, model_name, X_text_test, X_num_test, y_test, class_names):\n        \"\"\"Evaluate model performance\"\"\"\n        print(f\"\\nEvaluating {model_name}...\")\n\n        # Make predictions - handle MLP vs other models differently\n        if 'MLP' in model_name:\n            # MLP expects single input array\n            y_pred_proba = model.predict(X_text_test)  # X_text_test actually contains all features for MLP\n        else:\n            # Other models expect separate text and numerical inputs\n            y_pred_proba = model.predict([X_text_test, X_num_test])\n\n        # Make predictions\n        y_pred = np.argmax(y_pred_proba, axis=1)\n\n        # Calculate metrics\n        accuracy = accuracy_score(y_test, y_pred)\n        f1_weighted = f1_score(y_test, y_pred, average='weighted')\n        f1_macro = f1_score(y_test, y_pred, average='macro')\n        micro_precision = precision_score(y_test, y_pred, average='micro')\n        micro_recall = recall_score(y_test, y_pred, average='micro')\n\n        # Multi-class ROC AUC\n        try:\n            auc_score = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='weighted')\n        except:\n            auc_score = 0.0\n\n        # Precision, Recall, F1 per class\n        precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred, average=None)\n\n        # Classification report\n        report = classification_report(y_test, y_pred, target_names=class_names, output_dict=True)\n\n        # Confusion matrix\n        cm = confusion_matrix(y_test, y_pred)\n\n        results = {\n            'model_name': model_name,\n            'accuracy': accuracy,\n            'f1_weighted': f1_weighted,\n            'f1_macro': f1_macro,\n            'auc_score': auc_score,\n            'micro_precision': micro_precision,\n            'micro_recall': micro_recall,\n            'precision': precision,\n            'recall': recall,\n            'f1_per_class': f1,\n            'support': support,\n            'classification_report': report,\n            'confusion_matrix': cm,\n            'y_pred': y_pred,\n            'y_pred_proba': y_pred_proba\n        }\n\n        return results\n\n    def plot_training_history(self, model_name):\n        \"\"\"Plot training history\"\"\"\n        if model_name not in self.histories:\n            return\n\n        history = self.histories[model_name]\n\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n        # Plot accuracy\n        ax1.plot(history.history['accuracy'], label='Training Accuracy')\n        ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n        ax1.set_title(f'{model_name} - Accuracy')\n        ax1.set_xlabel('Epoch')\n        ax1.set_ylabel('Accuracy')\n        ax1.legend()\n        ax1.grid(True)\n\n        # Plot loss\n        ax2.plot(history.history['loss'], label='Training Loss')\n        ax2.plot(history.history['val_loss'], label='Validation Loss')\n        ax2.set_title(f'{model_name} - Loss')\n        ax2.set_xlabel('Epoch')\n        ax2.set_ylabel('Loss')\n        ax2.legend()\n        ax2.grid(True)\n\n        plt.tight_layout()\n        plt.savefig(f'charts/{model_name.lower().replace(\" \", \"_\")}_training_history.png',\n                    dpi=300, bbox_inches='tight')\n        plt.close()\n\n    def plot_confusion_matrix(self, results, class_names):\n        \"\"\"Plot confusion matrix\"\"\"\n        cm = results['confusion_matrix']\n        model_name = results['model_name']\n\n        plt.figure(figsize=(8, 6))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                    xticklabels=class_names, yticklabels=class_names)\n        plt.title(f'{model_name} - Confusion Matrix')\n        plt.ylabel('Actual')\n        plt.xlabel('Predicted')\n        plt.tight_layout()\n        plt.savefig(f'charts/{model_name.lower().replace(\" \", \"_\")}_confusion_matrix.png',\n                    dpi=300, bbox_inches='tight')\n        plt.close()\n\n    def save_all_models(self, all_results):\n        \"\"\"Save all trained models with their evaluation results and supporting files for API use\"\"\"\n        # Create directories if they don't exist\n        os.makedirs('api_models', exist_ok=True)\n        os.makedirs('api_models/data', exist_ok=True)\n\n        # Actual features from your Temu reviews dataset\n        feature_columns = [\n            'ReviewCount', 'UserCountry_encoded',\n            'text_length', 'word_count', 'avg_word_length',\n            'exclamation_count', 'question_count', 'upper_case_ratio',\n            'title_text_length', 'title_word_count', 'title_avg_word_length',\n            'title_exclamation_count', 'title_question_count', 'title_upper_case_ratio'\n        ]\n\n        # Class names based on ReviewRating (1-5 stars)\n        class_names = [\n            '1 Star - Very Poor',\n            '2 Stars - Poor',\n            '3 Stars - Average',\n            '4 Stars - Good',\n            '5 Stars - Excellent'\n        ]\n\n        # Create a package for each model that contains everything needed for serving\n        for result in all_results:\n            model_name = result['model_name']\n            if model_name in self.models:\n                # Create a directory for this model\n                model_dir = os.path.join('api_models', model_name.lower().replace(' ', '_'))\n                os.makedirs(model_dir, exist_ok=True)\n\n                # 1. Save the model in SavedModel format\n                model_path = os.path.join(model_dir, 'model.keras')\n                self.models[model_name].save(model_path)\n\n                # 2. Save metadata needed for preprocessing\n                metadata = {\n                    'max_sequence_length': self.max_len,\n                    'feature_columns': feature_columns,\n                    'class_names': class_names,\n                    'input_details': {\n                        'text_input': {\n                            'shape': [None, self.max_len],\n                            'dtype': 'int32',\n                            'description': 'Tokenized review text from ReviewText column'\n                        },\n                        'numerical_input': {\n                            'shape': [None, len(feature_columns)],\n                            'dtype': 'float32',\n                            'description': f'Numerical features in order: {\", \".join(feature_columns)}'\n                        }\n                    },\n                    'output_details': {\n                        'description': 'Probability scores for each rating level (1-5 stars)',\n                        'class_order': class_names\n                    },\n                    'data_source': 'temu_reviews_cleaned.csv',\n                    'text_columns_used': ['ReviewText', 'ReviewTitle'],  # Which text columns were used\n                    'model_format': 'keras'  # Indicate the saved format\n                }\n\n                with open(os.path.join(model_dir, 'metadata.json'), 'w') as f:\n                    json.dump(metadata, f, indent=2)\n\n                print(f\"\u2705 Saved API-ready {model_name} package to {model_dir}\")\n\n                # Update the result with the path\n                result['api_model_path'] = model_dir\n            else:\n                print(f\"\u26a0\ufe0f Model {model_name} not found in trained models\")\n\n        # Save tokenizer if exists\n        if hasattr(self, 'tokenizer'):\n            tokenizer_path = os.path.join('api_models', 'tokenizer.pkl')\n            with open(tokenizer_path, 'wb') as f:\n                pickle.dump(self.tokenizer, f)\n            print(f\"\u2705 Saved tokenizer to {tokenizer_path}\")\n\n        # Save complete results\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        results_path = os.path.join('api_models', 'data', f'model_results_{timestamp}.pkl')\n        with open(results_path, 'wb') as f:\n            pickle.dump(all_results, f)\n\n        print(f\"\\nAll models saved in API-ready format.\")\n        print(f\"You can now deploy any model by copying its directory to your API server.\")\n        return results_path\n\n\n\n\n\n# Define models to train\n#model_configs = [\n#    ('Deep MLP with TF-IDF', lambda: model_builder.create_mlp_model(X_train_mlp.shape[1])),\n#    ('LSTM Model', model_builder.create_lstm_model),\n#    ('BiLSTM with Attention', model_builder.create_robust_model),\n#    ('CNN Model', model_builder.create_cnn_model),\n#    ('Transformer Model', model_builder.create_transformer_model),\n#    ('Hybrid CNN-LSTM', model_builder.create_hybrid_cnn_lstm_model)\n#]\n\n\n# Initialize model builder\nmodel_builder = DeepLearningModels(vocab_size, max_len, num_features, num_classes, embedding_dim=128)\n\n#'Deep MLP with TF-IDF'\n# Define models to train\nmodel_configs = [\n    ('Deep MLP with TF-IDF', lambda: model_builder.create_mlp_model(X_train_mlp.shape[1]))\n]\n\n# Train and evaluate all models\nall_results = []\n\nfor model_name, model_func in model_configs:\n    print(f\"\\n{'='*50}\")\n    print(f\"Building and training {model_name}\")\n    print('='*50)\n    \n    # Create and compile model\n    model = model_func()\n    model = model_builder.compile_model(model)\n    \n    print(f\"\\n{model_name} Architecture:\")\n    model.summary()\n\n\n    # Special handling for MLP model (uses different data)\n    if 'MLP' in model_name:\n        # Handle class imbalance for MLP data\n        smote = SMOTE(random_state=42)\n        X_train_mlp_balanced, y_train_mlp_balanced = smote.fit_resample(X_train_mlp, y_train_mlp)\n\n        # Train MLP model\n        model, history = model_builder.train_model_mlp(\n            model, model_name,\n            X_train_mlp_balanced, X_val_mlp, y_train_mlp_balanced, y_val_mlp,\n            class_weights, epochs=50, batch_size=64\n        )\n\n        # Evaluate MLP model\n        results = model_builder.evaluate_model(\n            model, model_name, X_test_mlp, None, y_test_mlp, class_names\n        )\n    else:\n        # Train sequence-based models\n        model, history = model_builder.train_model(\n            model, model_name,\n            X_text_train_balanced, X_num_train_balanced, y_train_balanced,\n            X_text_val, X_num_val, y_val,\n            class_weights, epochs=50\n        )\n        # Evaluate model\n        results = model_builder.evaluate_model(\n            model, model_name, X_text_test, X_num_test, y_test, class_names\n        )\n    \n    # Plot training history\n    model_builder.plot_training_history(model_name)\n    \n    # Plot confusion matrix\n    model_builder.plot_confusion_matrix(results, class_names)\n    \n    all_results.append(results)\n    \n    # Print results\n    print(f\"\\n{model_name} Results:\")\n    print(f\"Accuracy: {results['accuracy']:.4f}\")\n    print(f\"F1-Score (Weighted): {results['f1_weighted']:.4f}\")\n    print(f\"F1-Score (Macro): {results['f1_macro']:.4f}\")\n    print(f\"AUC Score: {results['auc_score']:.4f}\")\n    print(results.keys())  # Check available keys\n    print(f\"Micro-averaged precision: {results['micro_precision']:.4f}\")\n    print(f\"Micro-average recall: {results['micro_recall']:.4f}\")\n    print(\"Precision:\", \", \".join([f\"{x:.4f}\" for x in results['precision']]))\n    print(\"Recall:\", \", \".join([f\"{x:.4f}\" for x in results['recall']]))\n    \n    # Save results\n    with open('data/model_results.pkl', 'wb') as f:\n        pickle.dump(all_results, f)\n    \n\n\n#model_configs = [\n#    ('Deep MLP with TF-IDF', lambda: model_builder.create_mlp_model(X_train_mlp.shape[1])),\n#    ('LSTM Model', model_builder.create_lstm_model),\n#    ('BiLSTM with Attention', model_builder.create_robust_model),\n#    ('CNN Model', model_builder.create_cnn_model),\n#    ('Transformer Model', model_builder.create_transformer_model),\n#    ('Hybrid CNN-LSTM', model_builder.create_hybrid_cnn_lstm_model)\n#]\n\n\n# Initialize model builder\nmodel_builder = DeepLearningModels(vocab_size, max_len, num_features, num_classes)\n\n#'BiLSTM with Attention', model_builder.create_bilstm_attention_model)\n# Define models to train\nmodel_configs = [\n    ('LSTM Model', model_builder.create_lstm_model)\n]\n\n# Train and evaluate all models\nfor model_name, model_func in model_configs:\n    print(f\"\\n{'='*50}\")\n    print(f\"Building and training {model_name}\")\n    print('='*50)\n    \n    # Create and compile model\n    model = model_func()\n    model = model_builder.compile_model(model)\n    \n    print(f\"\\n{model_name} Architecture:\")\n    model.summary()\n    \n    # Train model\n    model, history = model_builder.train_model(\n        model, model_name,\n        X_text_train_balanced, X_num_train_balanced, y_train_balanced,\n        X_text_val, X_num_val, y_val,\n        class_weights, epochs=50\n    )\n    \n    # Plot training history\n    model_builder.plot_training_history(model_name)\n    \n    # Evaluate model\n    results = model_builder.evaluate_model(\n        model, model_name, X_text_test, X_num_test, y_test, class_names\n    )\n    \n    # Plot confusion matrix\n    model_builder.plot_confusion_matrix(results, class_names)\n    \n    all_results.append(results)\n    \n    # Print results\n    print(f\"\\n{model_name} Results:\")\n    print(f\"Accuracy: {results['accuracy']:.4f}\")\n    print(f\"F1-Score (Weighted): {results['f1_weighted']:.4f}\")\n    print(f\"F1-Score (Macro): {results['f1_macro']:.4f}\")\n    print(f\"AUC Score: {results['auc_score']:.4f}\")\n    \n    # Save results\n    with open('data/model_results.pkl', 'wb') as f:\n        pickle.dump(all_results, f)\n    \n\n\n\n#model_configs = [\n#    ('Deep MLP with TF-IDF', lambda: model_builder.create_mlp_model(X_train_mlp.shape[1])),\n#    ('LSTM Model', model_builder.create_lstm_model),\n#    ('BiLSTM with Attention', model_builder.create_robust_model),\n#    ('CNN Model', model_builder.create_cnn_model),\n#    ('Transformer Model', model_builder.create_transformer_model),\n#    ('Hybrid CNN-LSTM', model_builder.create_hybrid_cnn_lstm_model)\n#]\n\n\n# Initialize model builder\nmodel_builder = DeepLearningModels(vocab_size, max_len, num_features, num_classes)\n\n#'CNN Model', model_builder.create_cnn_model)\n# Define models to train\nmodel_configs = [\n    ('BiLSTM with Attention', model_builder.create_robust_model)\n]\n\n# Train and evaluate all models\n\nfor model_name, model_func in model_configs:\n    print(f\"\\n{'='*50}\")\n    print(f\"Building and training {model_name}\")\n    print('='*50)\n    \n    # Create and compile model\n    model = model_func()\n    model = model_builder.compile_model(model)\n    \n    print(f\"\\n{model_name} Architecture:\")\n    model.summary()\n    \n    # Train model\n    model, history = model_builder.train_model(\n        model, model_name,\n        X_text_train_balanced, X_num_train_balanced, y_train_balanced,\n        X_text_val, X_num_val, y_val,\n        class_weights, epochs=50\n    )\n    \n    # Plot training history\n    model_builder.plot_training_history(model_name)\n    \n    # Evaluate model\n    results = model_builder.evaluate_model(\n        model, model_name, X_text_test, X_num_test, y_test, class_names\n    )\n    \n    # Plot confusion matrix\n    model_builder.plot_confusion_matrix(results, class_names)\n    \n    all_results.append(results)\n    \n    # Print results\n    print(f\"\\n{model_name} Results:\")\n    print(f\"Accuracy: {results['accuracy']:.4f}\")\n    print(f\"F1-Score (Weighted): {results['f1_weighted']:.4f}\")\n    print(f\"F1-Score (Macro): {results['f1_macro']:.4f}\")\n    print(f\"AUC Score: {results['auc_score']:.4f}\")\n    \n    # Save results\n    with open('data/model_results.pkl', 'wb') as f:\n        pickle.dump(all_results, f)\n    \n\n\n\n#model_configs = [\n#    ('Deep MLP with TF-IDF', lambda: model_builder.create_mlp_model(X_train_mlp.shape[1])),\n#    ('LSTM Model', model_builder.create_lstm_model),\n#    ('BiLSTM with Attention', model_builder.create_robust_model),\n#    ('CNN Model', model_builder.create_cnn_model),\n#    ('Transformer Model', model_builder.create_transformer_model),\n#    ('Hybrid CNN-LSTM', model_builder.create_hybrid_cnn_lstm_model)\n#]\n\n\n# Initialize model builder\nmodel_builder = DeepLearningModels(vocab_size, max_len, num_features, num_classes)\n\n#'Transformer Model', model_builder.create_transformer_model)\n# Define models to train\nmodel_configs = [\n    ('CNN Model', model_builder.create_cnn_model)\n]\n\n# Train and evaluate all models\n\nfor model_name, model_func in model_configs:\n    print(f\"\\n{'='*50}\")\n    print(f\"Building and training {model_name}\")\n    print('='*50)\n    \n    # Create and compile model\n    model = model_func()\n    model = model_builder.compile_model(model)\n    \n    print(f\"\\n{model_name} Architecture:\")\n    model.summary()\n    \n    # Train model\n    model, history = model_builder.train_model(\n        model, model_name,\n        X_text_train_balanced, X_num_train_balanced, y_train_balanced,\n        X_text_val, X_num_val, y_val,\n        class_weights, epochs=50\n    )\n    \n    # Plot training history\n    model_builder.plot_training_history(model_name)\n    \n    # Evaluate model\n    results = model_builder.evaluate_model(\n        model, model_name, X_text_test, X_num_test, y_test, class_names\n    )\n    \n    # Plot confusion matrix\n    model_builder.plot_confusion_matrix(results, class_names)\n    \n    all_results.append(results)\n    \n    # Print results\n    print(f\"\\n{model_name} Results:\")\n    print(f\"Accuracy: {results['accuracy']:.4f}\")\n    print(f\"F1-Score (Weighted): {results['f1_weighted']:.4f}\")\n    print(f\"F1-Score (Macro): {results['f1_macro']:.4f}\")\n    print(f\"AUC Score: {results['auc_score']:.4f}\")\n    \n    # Save results\n    with open('data/model_results.pkl', 'wb') as f:\n        pickle.dump(all_results, f)\n    \n\n\n\n#model_configs = [\n#    ('Deep MLP with TF-IDF', lambda: model_builder.create_mlp_model(X_train_mlp.shape[1])),\n#    ('LSTM Model', model_builder.create_lstm_model),\n#    ('BiLSTM with Attention', model_builder.create_robust_model),\n#    ('CNN Model', model_builder.create_cnn_model),\n#    ('Transformer Model', model_builder.create_transformer_model),\n#    ('Hybrid CNN-LSTM', model_builder.create_hybrid_cnn_lstm_model)\n#]\n\n\n# Initialize model builder\nmodel_builder = DeepLearningModels(vocab_size, max_len, num_features, num_classes)\n\n#'Hybrid CNN-LSTM', model_builder.create_hybrid_cnn_lstm_model)\n# Define models to train\nmodel_configs = [\n    ('Transformer Model', model_builder.create_transformer_model)\n]\n\n# Train and evaluate all models\n\nfor model_name, model_func in model_configs:\n    print(f\"\\n{'='*50}\")\n    print(f\"Building and training {model_name}\")\n    print('='*50)\n    \n    # Create and compile model\n    model = model_func()\n    model = model_builder.compile_model(model)\n    \n    print(f\"\\n{model_name} Architecture:\")\n    model.summary()\n    \n    # Train model\n    model, history = model_builder.train_model(\n        model, model_name,\n        X_text_train_balanced, X_num_train_balanced, y_train_balanced,\n        X_text_val, X_num_val, y_val,\n        class_weights, epochs=50\n    )\n    \n    # Plot training history\n    model_builder.plot_training_history(model_name)\n    \n    # Evaluate model\n    results = model_builder.evaluate_model(\n        model, model_name, X_text_test, X_num_test, y_test, class_names\n    )\n    \n    # Plot confusion matrix\n    model_builder.plot_confusion_matrix(results, class_names)\n    \n    all_results.append(results)\n    \n    # Print results\n    print(f\"\\n{model_name} Results:\")\n    print(f\"Accuracy: {results['accuracy']:.4f}\")\n    print(f\"F1-Score (Weighted): {results['f1_weighted']:.4f}\")\n    print(f\"F1-Score (Macro): {results['f1_macro']:.4f}\")\n    print(f\"AUC Score: {results['auc_score']:.4f}\")\n    \n    # Save results\n    with open('data/model_results.pkl', 'wb') as f:\n        pickle.dump(all_results, f)\n    \n\n\n\n#model_configs = [\n#    ('Deep MLP with TF-IDF', lambda: model_builder.create_mlp_model(X_train_mlp.shape[1])),\n#    ('LSTM Model', model_builder.create_lstm_model),\n#    ('BiLSTM with Attention', model_builder.create_robust_model),\n#    ('CNN Model', model_builder.create_cnn_model),\n#    ('Transformer Model', model_builder.create_transformer_model),\n#    ('Hybrid CNN-LSTM', model_builder.create_hybrid_cnn_lstm_model)\n#]\n\n\n# Initialize model builder\nmodel_builder = DeepLearningModels(vocab_size, max_len, num_features, num_classes)\n\n#'Hybrid CNN-LSTM', model_builder.create_hybrid_cnn_lstm_model)\n# Define models to train\nmodel_configs = [\n    ('Hybrid CNN-LSTM', model_builder.create_hybrid_cnn_lstm_model)\n]\n\n# Train and evaluate all models\n\nfor model_name, model_func in model_configs:\n    print(f\"\\n{'='*50}\")\n    print(f\"Building and training {model_name}\")\n    print('='*50)\n    \n    # Create and compile model\n    model = model_func()\n    model = model_builder.compile_model(model)\n    \n    print(f\"\\n{model_name} Architecture:\")\n    model.summary()\n    \n    # Train model\n    model, history = model_builder.train_model(\n        model, model_name,\n        X_text_train_balanced, X_num_train_balanced, y_train_balanced,\n        X_text_val, X_num_val, y_val,\n        class_weights, epochs=50\n    )\n    \n    # Plot training history\n    model_builder.plot_training_history(model_name)\n    \n    # Evaluate model\n    results = model_builder.evaluate_model(\n        model, model_name, X_text_test, X_num_test, y_test, class_names\n    )\n    \n    # Plot confusion matrix\n    model_builder.plot_confusion_matrix(results, class_names)\n    \n    all_results.append(results)\n    \n    # Print results\n    print(f\"\\n{model_name} Results:\")\n    print(f\"Accuracy: {results['accuracy']:.4f}\")\n    print(f\"F1-Score (Weighted): {results['f1_weighted']:.4f}\")\n    print(f\"F1-Score (Macro): {results['f1_macro']:.4f}\")\n    print(f\"AUC Score: {results['auc_score']:.4f}\")\n    \n    # Save results\n    with open('data/model_results.pkl', 'wb') as f:\n        pickle.dump(all_results, f)\n    \n\n\n\n# Create comparison summary\nsummary_data = []\nfor result in all_results:\n    summary_data.append({\n        'Model': result['model_name'],\n        'Accuracy': result['accuracy'],\n        'F1-Weighted': result['f1_weighted'],\n        'F1-Macro': result['f1_macro'],\n        'AUC Score': result['auc_score']\n    })\n\nsummary_df = pd.DataFrame(summary_data)\nprint(all_results)\n#summary_df = summary_df.sort_values('F1-Weighted', ascending=False)\n\n\n# Create comparison summary\nsummary_data = []\nfor result in all_results:\n    summary_data.append({\n        'Model': result['model_name'],\n        'Accuracy': result['accuracy'],\n        'F1-Weighted': result['f1_weighted'],\n        'F1-Macro': result['f1_macro'],\n        'AUC Score': result['auc_score']\n    })\n\nsummary_df = pd.DataFrame(summary_data)\nsummary_df = summary_df.sort_values('F1-Weighted', ascending=False)\n\nprint(f\"\\n{'='*80}\")\nprint(\"MODEL PERFORMANCE COMPARISON\")\nprint('='*80)\nprint(summary_df.to_string(index=False, float_format='%.4f'))\n\n# Save summary\nsummary_df.to_csv('data/model_comparison_summary.csv', index=False)\n\n# Plot comparison\nplt.figure(figsize=(14, 8))\n\nmetrics = ['Accuracy', 'F1-Weighted', 'F1-Macro', 'AUC Score']\nx = np.arange(len(summary_df))\nwidth = 0.2\n\nfor i, metric in enumerate(metrics):\n    plt.bar(x + i*width, summary_df[metric], width, label=metric, alpha=0.8)\n\nplt.xlabel('Models')\nplt.ylabel('Score')\nplt.title('Deep Learning Models Performance Comparison')\nplt.xticks(x + width*1.5, summary_df['Model'], rotation=45, ha='right')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('charts/model_comparison.png', dpi=300, bbox_inches='tight')\nplt.close()\n\nprint(f\"\\nBest performing model: {summary_df.iloc[0]['Model']}\")\nprint(f\"Best F1-Weighted Score: {summary_df.iloc[0]['F1-Weighted']:.4f}\")\n\n# After training all models, save everything\nresults_path = model_builder.save_all_models(all_results)\nprint(f\"All models and results saved. Results path: {results_path}\")\n    \n    \n\n\n\n"
    try:
        exec(code, env)
    except Exception as e:
        st.error("Error while executing notebook code. Check the console logs.")
        st.exception(e)
        raise
    return env

env = load_env()

tab_run, tab_code = st.tabs(["▶️ Run functions", "📄 Notebook source"])

with tab_code:
    st.subheader("Notebook Code")
    st.code(r"#!/usr/bin/env python3\n\"\"\"\nDeep Learning Models for Customer Satisfaction Prediction\nImplements and compares 6 different deep learning architectures\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score\nfrom sklearn.metrics import precision_recall_fscore_support, recall_score, precision_score\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model, Sequential\n\nfrom nltk.corpus import wordnet\nfrom tensorflow.keras.layers import (\n    Dense, LSTM, Bidirectional, Conv1D, MaxPooling1D, GlobalMaxPooling1D,\n    Embedding, Dropout, Input, concatenate, Attention, MultiHeadAttention,\n    LayerNormalization, Add, GlobalAveragePooling1D, BatchNormalization, SpatialDropout1D\n)\n\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom tensorflow.keras.callbacks import (\n    EarlyStopping,\n    ReduceLROnPlateau,\n    TerminateOnNaN,\n    ModelCheckpoint,\n    CSVLogger\n)\n\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.utils import to_categorical\nfrom imblearn.over_sampling import SMOTE\nimport pickle\nimport warnings\nimport os\nimport json\nfrom datetime import datetime\nwarnings.filterwarnings('ignore')\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Load preprocessed data\nprint(\"Loading preprocessed data...\")\ndata = np.load('data/preprocessed_data.npz')\n\nX_num_train = data['X_num_train']\nX_num_val = data['X_num_val']\nX_num_test = data['X_num_test']\nX_text_train = data['X_text_train']\nX_text_val = data['X_text_val']\nX_text_test = data['X_text_test']\nX_num_train_balanced = data['X_num_train_balanced']\nX_text_train_balanced = data['X_text_train_balanced']\ny_train = data['y_train']\ny_val = data['y_val']\ny_test = data['y_test']\ny_train_balanced = data['y_train_balanced']\n\n# Load TF-IDF based data\nX_train_mlp = data['X_train_mlp']\nX_val_mlp = data['X_val_mlp']\nX_test_mlp = data['X_test_mlp']\ny_train_mlp = data['y_train_mlp']\ny_val_mlp = data['y_val_mlp']\ny_test_mlp = data['y_test_mlp']\n\n# Load metadata\nwith open('data/metadata.pkl', 'rb') as f:\n    metadata = pickle.load(f)\n\nvocab_size = metadata['vocab_size']\nmax_len = metadata['max_sequence_length']\nnum_features = len(metadata['feature_columns'])\nnum_classes = metadata['num_classes']\nclass_weights = metadata['class_weights']\nclass_names = metadata['class_names']\n\nprint(f\"Vocab size: {vocab_size}\")\nprint(f\"Max sequence length: {max_len}\")\nprint(f\"Number of features: {num_features}\")\nprint(f\"Number of classes: {num_classes}\")\nprint(f\"Training with balanced data: {X_text_train_balanced.shape[0]} samples\")\n\n\n\nclass DeepLearningModels:\n    def __init__(self, vocab_size, max_len, num_features, num_classes, embedding_dim=128):\n        self.vocab_size = vocab_size\n        self.max_len = max_len\n        self.num_features = num_features\n        self.num_classes = num_classes\n        self.embedding_dim = embedding_dim\n        self.models = {}\n        self.histories = {}\n\n    def create_mlp_model(self, input_dim):\n        \"\"\"Model 6: Deep MLP with TF-IDF features\"\"\"\n        model = Sequential([ Input(shape=(input_dim,)),\n                  # First hidden layer with batch normalization\n                  Dense(1024, activation='relu', kernel_regularizer=l2(0.001), input_shape=(input_dim,)),\n                  BatchNormalization(),\n                  Dropout(0.6),\n                  # Second hidden layer\n                  Dense(512, activation='relu', kernel_regularizer=l2(0.001)),\n                  BatchNormalization(),\n                  Dropout(0.5),\n                  # Third hidden layer\n                  Dense(256, activation='relu'),\n                  Dropout(0.4),\n                  Dense(128, activation='relu'),\n                  # Output layer\n                  Dense(self.num_classes, activation='softmax')\n        ])\n        return model\n\n\n    def create_lstm_model(self):\n        \"\"\"Model 1: LSTM-based RNN for sequential text processing\"\"\"\n        # Text input branch\n        text_input = Input(shape=(self.max_len,))\n        x = Embedding(self.vocab_size, self.embedding_dim, mask_zero=True)(text_input)\n        x = SpatialDropout1D(0.3)(x)\n        x = LSTM(128, return_sequences=True)(x)\n        x = LSTM(64)(x)\n\n        # Numerical input branch\n        num_input = Input(shape=(self.num_features,))\n        y = Dense(64, activation='relu')(num_input)\n        y = BatchNormalization()(y)\n\n        # Combine branches\n        z = concatenate([x, y])\n        z = Dense(128, activation='relu')(z)\n        z = Dropout(0.5)(z)\n        output = Dense(self.num_classes, activation='softmax')(z)\n\n        model = Model(inputs=[text_input, num_input], outputs=output)\n        optimizer = Adam(learning_rate=0.001, clipvalue=0.5)\n        model.compile(optimizer=optimizer,\n                      loss='sparse_categorical_crossentropy',\n                      metrics=['accuracy'])\n        return model\n\n    def create_bilstm_attention_model(self):\n        \"\"\"Model 2: Bidirectional LSTM with attention mechanism\"\"\"\n        # Text input branch\n        text_input = Input(shape=(self.max_len,))\n        x = Embedding(self.vocab_size, self.embedding_dim, mask_zero=True)(text_input)\n        x = Bidirectional(LSTM(128, return_sequences=True))(x)\n\n        # Attention\n        attention = MultiHeadAttention(num_heads=4, key_dim=64)(x, x)\n        x = LayerNormalization()(x + attention)\n\n        # Numerical branch\n        num_input = Input(shape=(self.num_features,))\n        y = Dense(64, activation='swish')(num_input)\n        y = LayerNormalization()(y)\n\n        # Combined\n        context = GlobalAveragePooling1D()(x)\n        z = concatenate([context, y])\n        z = Dense(256, activation='swish')(z)\n        z = Dropout(0.4)(z)\n        output = Dense(self.num_classes, activation='softmax')(z)\n\n        model = Model(inputs=[text_input, num_input], outputs=output)\n        model.compile(optimizer=Adam(learning_rate=0.0005),\n                      loss='sparse_categorical_crossentropy',\n                      metrics=['accuracy'])\n        return model\n\n    def create_robust_model(self):\n        # Text\n        text_input = Input(shape=(self.max_len,))\n        x = Embedding(self.vocab_size, 128)(text_input)\n        x = Bidirectional(LSTM(64))(x)\n\n        # Numerical\n        num_input = Input(shape=(self.num_features,))\n        y = Dense(64)(num_input)\n\n        # Combined\n        z = concatenate([x, y])\n        z = Dense(128, activation='relu')(z)\n        output = Dense(self.num_classes, activation='softmax')(z)\n\n        model = Model(inputs=[text_input, num_input], outputs=output)\n        model.compile(\n            optimizer='adam',\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        return model\n\n    def create_accurate_bilstm_attention_model(self):\n        \"\"\"High-performance BiLSTM with Attention\"\"\"\n        # Text input branch\n        text_input = Input(shape=(self.max_len,), name='text_input')\n        text_embedding = Embedding(\n            self.vocab_size,\n            self.embedding_dim * 2,  # Increased capacity\n            mask_zero=True\n        )(text_input)\n\n        # Enhanced Bidirectional LSTM\n        bilstm = Bidirectional(\n            LSTM(128,  # Doubled units\n                 dropout=0.3,\n                 recurrent_dropout=0.25,\n                 return_sequences=True,\n                 kernel_regularizer=l2(1e-4))  # Added regularization\n        )(text_embedding)\n        bilstm = BatchNormalization()(bilstm)  # Stabilizes training\n\n        # Powerful attention mechanism\n        attention = MultiHeadAttention(\n            num_heads=8,  # More attention heads\n            key_dim=128,  # Matches LSTM units\n            dropout=0.2,\n            kernel_regularizer=l2(1e-4)\n        )(bilstm, bilstm)\n\n        # Residual connection with layer norm\n        attention = Add()([bilstm, attention])\n        attention = LayerNormalization()(attention)\n\n        # Context extraction\n        text_features = GlobalAveragePooling1D()(attention)\n\n        # Enhanced numerical branch\n        num_input = Input(shape=(self.num_features,), name='numerical_input')\n        num_dense = Dense(64, activation='relu', kernel_regularizer=l2(1e-4))(num_input)\n        num_dense = BatchNormalization()(num_dense)\n\n        # Feature fusion\n        combined = concatenate([text_features, num_dense])\n        combined = Dropout(0.4)(combined)\n\n        # Deep classifier head\n        hidden = Dense(256, activation='relu', kernel_regularizer=l2(1e-4))(combined)\n        hidden = BatchNormalization()(hidden)\n        hidden = Dropout(0.4)(hidden)\n        hidden = Dense(128, activation='relu')(hidden)\n        output = Dense(self.num_classes, activation='softmax')(hidden)\n\n        model = Model(inputs=[text_input, num_input], outputs=output)\n        return model\n\n    def create_cnn_model(self):\n        \"\"\"Model 3: CNN for text classification with multiple filter sizes\"\"\"\n        # Text input branch\n        text_input = Input(shape=(self.max_len,))\n        x = Embedding(self.vocab_size, 128)(text_input)\n\n        # Parallel conv branches with residual connections\n        convs = []\n        for filter_size in [3, 5, 7]:\n            conv = Conv1D(128, filter_size, padding='same', activation='relu')(x)\n            conv = MaxPooling1D(2)(conv)\n            conv = Conv1D(64, filter_size, padding='same', activation='relu')(conv)\n            conv = GlobalMaxPooling1D()(conv)\n            convs.append(conv)\n\n        x = concatenate(convs) if len(convs) > 1 else convs[0]\n\n        # Numerical branch\n        num_input = Input(shape=(self.num_features,))\n        y = Dense(64, activation='relu')(num_input)\n        y = BatchNormalization()(y)\n\n        # Combined\n        z = concatenate([x, y])\n        z = Dense(128, activation='relu')(z)\n        z = Dropout(0.5)(z)\n        output = Dense(self.num_classes, activation='softmax')(z)\n\n        model = Model(inputs=[text_input, num_input], outputs=output)\n        model.compile(optimizer=RMSprop(learning_rate=0.0005),\n                      loss='sparse_categorical_crossentropy',\n                      metrics=['accuracy'])\n        return model\n\n    def create_transformer_model(self):\n        \"\"\"Model 4: Transformer-based model (simplified BERT-like architecture)\"\"\"\n        # Text input branch\n        text_input = Input(shape=(self.max_len,))\n        x = Embedding(self.vocab_size, 128)(text_input)\n\n        # Positional encoding\n        positions = tf.range(start=0, limit=self.max_len, delta=1)\n        positions = Embedding(self.max_len, 128)(positions)\n        x = x + positions\n\n        # Transformer blocks\n        for _ in range(3):  # Additional layer\n            attn = MultiHeadAttention(num_heads=4, key_dim=32)(x, x)\n            x = LayerNormalization()(x + attn)\n            ffn = Dense(512, activation='gelu')(x)\n            ffn = Dense(128)(ffn)\n            x = LayerNormalization()(x + ffn)\n\n        x = GlobalAveragePooling1D()(x)\n\n        # Numerical branch\n        num_input = Input(shape=(self.num_features,))\n        y = Dense(64, activation='relu')(num_input)\n        y = LayerNormalization()(y)\n\n        # Combined\n        z = concatenate([x, y])\n        z = Dense(256, activation='gelu')(z)\n        z = Dropout(0.4)(z)\n        output = Dense(self.num_classes, activation='softmax')(z)\n\n        model = Model(inputs=[text_input, num_input], outputs=output)\n        model.compile(optimizer=Adam(learning_rate=0.0001),\n                      loss='sparse_categorical_crossentropy',\n                      metrics=['accuracy'])\n        return model\n\n    def create_hybrid_cnn_lstm_model(self):\n        \"\"\"Model 5: Hybrid CNN-LSTM model\"\"\"\n        # Text input branch\n        text_input = Input(shape=(self.max_len,))\n        x = Embedding(self.vocab_size, 128)(text_input)\n\n        # CNN part\n        conv1 = Conv1D(128, 3, padding='same', activation='relu')(x)\n        conv1 = MaxPooling1D(2)(conv1)\n        conv2 = Conv1D(128, 5, padding='same', activation='relu')(x)\n        conv2 = MaxPooling1D(2)(conv2)\n        x = concatenate([conv1, conv2])\n        x = BatchNormalization()(x)\n\n        # LSTM part\n        x = Bidirectional(LSTM(128))(x)\n\n        # Numerical branch\n        num_input = Input(shape=(self.num_features,))\n        y = Dense(64, activation='relu')(num_input)\n        y = BatchNormalization()(y)\n\n        # Combined\n        z = concatenate([x, y])\n        z = Dense(256, activation='relu')(z)\n        z = Dropout(0.5)(z)\n        output = Dense(self.num_classes, activation='softmax')(z)\n\n        model = Model(inputs=[text_input, num_input], outputs=output)\n        model.compile(optimizer=Adam(learning_rate=0.0005),\n                      loss='sparse_categorical_crossentropy',\n                      metrics=['accuracy'])\n        return model\n\n    def create_hybrid_cnn_lstm_model_modified(self):\n        \"\"\"Fixed version with all required imports\"\"\"\n        # Text input branch\n        text_input = Input(shape=(self.max_len,), name='text_input')\n        text_embedding = Embedding(self.vocab_size, self.embedding_dim)(text_input)\n\n        # CNN with MaxPooling\n        conv1 = Conv1D(128, 3, activation='relu', padding='same')(text_embedding)\n        conv1 = MaxPooling1D(2)(conv1)\n        conv2 = Conv1D(128, 5, activation='relu', padding='same')(text_embedding)\n        conv2 = MaxPooling1D(2)(conv2)\n\n        conv_combined = concatenate([conv1, conv2])\n        conv_combined = BatchNormalization()(conv_combined)  # Now properly imported\n\n        # LSTM\n        lstm_out = LSTM(128, dropout=0.2, recurrent_dropout=0.2)(conv_combined)\n\n        # Numerical branch\n        num_input = Input(shape=(self.num_features,), name='numerical_input')\n        num_dense = Dense(32, activation='relu')(num_input)\n\n        # Combine branches\n        combined = concatenate([lstm_out, num_dense])\n        hidden = Dense(128, activation='relu')(combined)\n        output = Dense(self.num_classes, activation='softmax')(hidden)\n\n        model = Model(inputs=[text_input, num_input], outputs=output)\n        return model\n\n    def compile_model(self, model, learning_rate=0.001):\n        \"\"\"Compile model with appropriate optimizer and loss function\"\"\"\n        model.compile(\n            optimizer=Adam(learning_rate=learning_rate),\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        return model\n\n    def create_callbacks(self):\n        \"\"\"Create training callbacks\"\"\"\n        early_stopping = EarlyStopping(\n            monitor='val_loss',\n            patience=15,\n            restore_best_weights=True\n        )\n\n        reduce_lr = ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=7,\n            min_lr=1e-7\n        )\n\n        return [early_stopping, reduce_lr]\n\n    def train_model_mlp(self, model, model_name, X_train, X_val, y_train, y_val,\n                    class_weights=None, epochs=100, batch_size=128):\n        \"\"\"Train a model with given data\"\"\"\n        print(f\"\\nTraining {model_name}...\")\n\n        #callbacks = self.create_callbacks()\n        callbacks = [\n            EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-5),\n            TerminateOnNaN()\n        ]\n\n        history = model.fit(\n            X_train, y_train,\n            validation_data=(X_val, y_val),\n            epochs=epochs,\n            batch_size=batch_size,\n            class_weight=class_weights,\n            callbacks=callbacks,\n            verbose=1\n        )\n\n        self.models[model_name] = model\n        self.histories[model_name] = history\n\n        return model, history\n\n    def train_model(self, model, model_name, X_text_train, X_num_train, y_train,\n                    X_text_val, X_num_val, y_val, class_weights, epochs=100):\n        \"\"\"Train a model with given data\"\"\"\n        print(f\"\\nTraining {model_name}...\")\n\n        callbacks = self.create_callbacks()\n\n        history = model.fit(\n            [X_text_train, X_num_train], y_train,\n            validation_data=([X_text_val, X_num_val], y_val),\n            epochs=epochs,\n            batch_size=16,\n            class_weight=class_weights,\n            callbacks=callbacks,\n            verbose=1\n        )\n\n        self.models[model_name] = model\n        self.histories[model_name] = history\n\n        return model, history\n\n    def evaluate_model(self, model, model_name, X_text_test, X_num_test, y_test, class_names):\n        \"\"\"Evaluate model performance\"\"\"\n        print(f\"\\nEvaluating {model_name}...\")\n\n        # Make predictions - handle MLP vs other models differently\n        if 'MLP' in model_name:\n            # MLP expects single input array\n            y_pred_proba = model.predict(X_text_test)  # X_text_test actually contains all features for MLP\n        else:\n            # Other models expect separate text and numerical inputs\n            y_pred_proba = model.predict([X_text_test, X_num_test])\n\n        # Make predictions\n        y_pred = np.argmax(y_pred_proba, axis=1)\n\n        # Calculate metrics\n        accuracy = accuracy_score(y_test, y_pred)\n        f1_weighted = f1_score(y_test, y_pred, average='weighted')\n        f1_macro = f1_score(y_test, y_pred, average='macro')\n        micro_precision = precision_score(y_test, y_pred, average='micro')\n        micro_recall = recall_score(y_test, y_pred, average='micro')\n\n        # Multi-class ROC AUC\n        try:\n            auc_score = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='weighted')\n        except:\n            auc_score = 0.0\n\n        # Precision, Recall, F1 per class\n        precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred, average=None)\n\n        # Classification report\n        report = classification_report(y_test, y_pred, target_names=class_names, output_dict=True)\n\n        # Confusion matrix\n        cm = confusion_matrix(y_test, y_pred)\n\n        results = {\n            'model_name': model_name,\n            'accuracy': accuracy,\n            'f1_weighted': f1_weighted,\n            'f1_macro': f1_macro,\n            'auc_score': auc_score,\n            'micro_precision': micro_precision,\n            'micro_recall': micro_recall,\n            'precision': precision,\n            'recall': recall,\n            'f1_per_class': f1,\n            'support': support,\n            'classification_report': report,\n            'confusion_matrix': cm,\n            'y_pred': y_pred,\n            'y_pred_proba': y_pred_proba\n        }\n\n        return results\n\n    def plot_training_history(self, model_name):\n        \"\"\"Plot training history\"\"\"\n        if model_name not in self.histories:\n            return\n\n        history = self.histories[model_name]\n\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n        # Plot accuracy\n        ax1.plot(history.history['accuracy'], label='Training Accuracy')\n        ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n        ax1.set_title(f'{model_name} - Accuracy')\n        ax1.set_xlabel('Epoch')\n        ax1.set_ylabel('Accuracy')\n        ax1.legend()\n        ax1.grid(True)\n\n        # Plot loss\n        ax2.plot(history.history['loss'], label='Training Loss')\n        ax2.plot(history.history['val_loss'], label='Validation Loss')\n        ax2.set_title(f'{model_name} - Loss')\n        ax2.set_xlabel('Epoch')\n        ax2.set_ylabel('Loss')\n        ax2.legend()\n        ax2.grid(True)\n\n        plt.tight_layout()\n        plt.savefig(f'charts/{model_name.lower().replace(\" \", \"_\")}_training_history.png',\n                    dpi=300, bbox_inches='tight')\n        plt.close()\n\n    def plot_confusion_matrix(self, results, class_names):\n        \"\"\"Plot confusion matrix\"\"\"\n        cm = results['confusion_matrix']\n        model_name = results['model_name']\n\n        plt.figure(figsize=(8, 6))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                    xticklabels=class_names, yticklabels=class_names)\n        plt.title(f'{model_name} - Confusion Matrix')\n        plt.ylabel('Actual')\n        plt.xlabel('Predicted')\n        plt.tight_layout()\n        plt.savefig(f'charts/{model_name.lower().replace(\" \", \"_\")}_confusion_matrix.png',\n                    dpi=300, bbox_inches='tight')\n        plt.close()\n\n    def save_all_models(self, all_results):\n        \"\"\"Save all trained models with their evaluation results and supporting files for API use\"\"\"\n        # Create directories if they don't exist\n        os.makedirs('api_models', exist_ok=True)\n        os.makedirs('api_models/data', exist_ok=True)\n\n        # Actual features from your Temu reviews dataset\n        feature_columns = [\n            'ReviewCount', 'UserCountry_encoded',\n            'text_length', 'word_count', 'avg_word_length',\n            'exclamation_count', 'question_count', 'upper_case_ratio',\n            'title_text_length', 'title_word_count', 'title_avg_word_length',\n            'title_exclamation_count', 'title_question_count', 'title_upper_case_ratio'\n        ]\n\n        # Class names based on ReviewRating (1-5 stars)\n        class_names = [\n            '1 Star - Very Poor',\n            '2 Stars - Poor',\n            '3 Stars - Average',\n            '4 Stars - Good',\n            '5 Stars - Excellent'\n        ]\n\n        # Create a package for each model that contains everything needed for serving\n        for result in all_results:\n            model_name = result['model_name']\n            if model_name in self.models:\n                # Create a directory for this model\n                model_dir = os.path.join('api_models', model_name.lower().replace(' ', '_'))\n                os.makedirs(model_dir, exist_ok=True)\n\n                # 1. Save the model in SavedModel format\n                model_path = os.path.join(model_dir, 'model.keras')\n                self.models[model_name].save(model_path)\n\n                # 2. Save metadata needed for preprocessing\n                metadata = {\n                    'max_sequence_length': self.max_len,\n                    'feature_columns': feature_columns,\n                    'class_names': class_names,\n                    'input_details': {\n                        'text_input': {\n                            'shape': [None, self.max_len],\n                            'dtype': 'int32',\n                            'description': 'Tokenized review text from ReviewText column'\n                        },\n                        'numerical_input': {\n                            'shape': [None, len(feature_columns)],\n                            'dtype': 'float32',\n                            'description': f'Numerical features in order: {\", \".join(feature_columns)}'\n                        }\n                    },\n                    'output_details': {\n                        'description': 'Probability scores for each rating level (1-5 stars)',\n                        'class_order': class_names\n                    },\n                    'data_source': 'temu_reviews_cleaned.csv',\n                    'text_columns_used': ['ReviewText', 'ReviewTitle'],  # Which text columns were used\n                    'model_format': 'keras'  # Indicate the saved format\n                }\n\n                with open(os.path.join(model_dir, 'metadata.json'), 'w') as f:\n                    json.dump(metadata, f, indent=2)\n\n                print(f\"\u2705 Saved API-ready {model_name} package to {model_dir}\")\n\n                # Update the result with the path\n                result['api_model_path'] = model_dir\n            else:\n                print(f\"\u26a0\ufe0f Model {model_name} not found in trained models\")\n\n        # Save tokenizer if exists\n        if hasattr(self, 'tokenizer'):\n            tokenizer_path = os.path.join('api_models', 'tokenizer.pkl')\n            with open(tokenizer_path, 'wb') as f:\n                pickle.dump(self.tokenizer, f)\n            print(f\"\u2705 Saved tokenizer to {tokenizer_path}\")\n\n        # Save complete results\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        results_path = os.path.join('api_models', 'data', f'model_results_{timestamp}.pkl')\n        with open(results_path, 'wb') as f:\n            pickle.dump(all_results, f)\n\n        print(f\"\\nAll models saved in API-ready format.\")\n        print(f\"You can now deploy any model by copying its directory to your API server.\")\n        return results_path\n\n\n\n\n\n# Define models to train\n#model_configs = [\n#    ('Deep MLP with TF-IDF', lambda: model_builder.create_mlp_model(X_train_mlp.shape[1])),\n#    ('LSTM Model', model_builder.create_lstm_model),\n#    ('BiLSTM with Attention', model_builder.create_robust_model),\n#    ('CNN Model', model_builder.create_cnn_model),\n#    ('Transformer Model', model_builder.create_transformer_model),\n#    ('Hybrid CNN-LSTM', model_builder.create_hybrid_cnn_lstm_model)\n#]\n\n\n# Initialize model builder\nmodel_builder = DeepLearningModels(vocab_size, max_len, num_features, num_classes, embedding_dim=128)\n\n#'Deep MLP with TF-IDF'\n# Define models to train\nmodel_configs = [\n    ('Deep MLP with TF-IDF', lambda: model_builder.create_mlp_model(X_train_mlp.shape[1]))\n]\n\n# Train and evaluate all models\nall_results = []\n\nfor model_name, model_func in model_configs:\n    print(f\"\\n{'='*50}\")\n    print(f\"Building and training {model_name}\")\n    print('='*50)\n    \n    # Create and compile model\n    model = model_func()\n    model = model_builder.compile_model(model)\n    \n    print(f\"\\n{model_name} Architecture:\")\n    model.summary()\n\n\n    # Special handling for MLP model (uses different data)\n    if 'MLP' in model_name:\n        # Handle class imbalance for MLP data\n        smote = SMOTE(random_state=42)\n        X_train_mlp_balanced, y_train_mlp_balanced = smote.fit_resample(X_train_mlp, y_train_mlp)\n\n        # Train MLP model\n        model, history = model_builder.train_model_mlp(\n            model, model_name,\n            X_train_mlp_balanced, X_val_mlp, y_train_mlp_balanced, y_val_mlp,\n            class_weights, epochs=50, batch_size=64\n        )\n\n        # Evaluate MLP model\n        results = model_builder.evaluate_model(\n            model, model_name, X_test_mlp, None, y_test_mlp, class_names\n        )\n    else:\n        # Train sequence-based models\n        model, history = model_builder.train_model(\n            model, model_name,\n            X_text_train_balanced, X_num_train_balanced, y_train_balanced,\n            X_text_val, X_num_val, y_val,\n            class_weights, epochs=50\n        )\n        # Evaluate model\n        results = model_builder.evaluate_model(\n            model, model_name, X_text_test, X_num_test, y_test, class_names\n        )\n    \n    # Plot training history\n    model_builder.plot_training_history(model_name)\n    \n    # Plot confusion matrix\n    model_builder.plot_confusion_matrix(results, class_names)\n    \n    all_results.append(results)\n    \n    # Print results\n    print(f\"\\n{model_name} Results:\")\n    print(f\"Accuracy: {results['accuracy']:.4f}\")\n    print(f\"F1-Score (Weighted): {results['f1_weighted']:.4f}\")\n    print(f\"F1-Score (Macro): {results['f1_macro']:.4f}\")\n    print(f\"AUC Score: {results['auc_score']:.4f}\")\n    print(results.keys())  # Check available keys\n    print(f\"Micro-averaged precision: {results['micro_precision']:.4f}\")\n    print(f\"Micro-average recall: {results['micro_recall']:.4f}\")\n    print(\"Precision:\", \", \".join([f\"{x:.4f}\" for x in results['precision']]))\n    print(\"Recall:\", \", \".join([f\"{x:.4f}\" for x in results['recall']]))\n    \n    # Save results\n    with open('data/model_results.pkl', 'wb') as f:\n        pickle.dump(all_results, f)\n    \n\n\n#model_configs = [\n#    ('Deep MLP with TF-IDF', lambda: model_builder.create_mlp_model(X_train_mlp.shape[1])),\n#    ('LSTM Model', model_builder.create_lstm_model),\n#    ('BiLSTM with Attention', model_builder.create_robust_model),\n#    ('CNN Model', model_builder.create_cnn_model),\n#    ('Transformer Model', model_builder.create_transformer_model),\n#    ('Hybrid CNN-LSTM', model_builder.create_hybrid_cnn_lstm_model)\n#]\n\n\n# Initialize model builder\nmodel_builder = DeepLearningModels(vocab_size, max_len, num_features, num_classes)\n\n#'BiLSTM with Attention', model_builder.create_bilstm_attention_model)\n# Define models to train\nmodel_configs = [\n    ('LSTM Model', model_builder.create_lstm_model)\n]\n\n# Train and evaluate all models\nfor model_name, model_func in model_configs:\n    print(f\"\\n{'='*50}\")\n    print(f\"Building and training {model_name}\")\n    print('='*50)\n    \n    # Create and compile model\n    model = model_func()\n    model = model_builder.compile_model(model)\n    \n    print(f\"\\n{model_name} Architecture:\")\n    model.summary()\n    \n    # Train model\n    model, history = model_builder.train_model(\n        model, model_name,\n        X_text_train_balanced, X_num_train_balanced, y_train_balanced,\n        X_text_val, X_num_val, y_val,\n        class_weights, epochs=50\n    )\n    \n    # Plot training history\n    model_builder.plot_training_history(model_name)\n    \n    # Evaluate model\n    results = model_builder.evaluate_model(\n        model, model_name, X_text_test, X_num_test, y_test, class_names\n    )\n    \n    # Plot confusion matrix\n    model_builder.plot_confusion_matrix(results, class_names)\n    \n    all_results.append(results)\n    \n    # Print results\n    print(f\"\\n{model_name} Results:\")\n    print(f\"Accuracy: {results['accuracy']:.4f}\")\n    print(f\"F1-Score (Weighted): {results['f1_weighted']:.4f}\")\n    print(f\"F1-Score (Macro): {results['f1_macro']:.4f}\")\n    print(f\"AUC Score: {results['auc_score']:.4f}\")\n    \n    # Save results\n    with open('data/model_results.pkl', 'wb') as f:\n        pickle.dump(all_results, f)\n    \n\n\n\n#model_configs = [\n#    ('Deep MLP with TF-IDF', lambda: model_builder.create_mlp_model(X_train_mlp.shape[1])),\n#    ('LSTM Model', model_builder.create_lstm_model),\n#    ('BiLSTM with Attention', model_builder.create_robust_model),\n#    ('CNN Model', model_builder.create_cnn_model),\n#    ('Transformer Model', model_builder.create_transformer_model),\n#    ('Hybrid CNN-LSTM', model_builder.create_hybrid_cnn_lstm_model)\n#]\n\n\n# Initialize model builder\nmodel_builder = DeepLearningModels(vocab_size, max_len, num_features, num_classes)\n\n#'CNN Model', model_builder.create_cnn_model)\n# Define models to train\nmodel_configs = [\n    ('BiLSTM with Attention', model_builder.create_robust_model)\n]\n\n# Train and evaluate all models\n\nfor model_name, model_func in model_configs:\n    print(f\"\\n{'='*50}\")\n    print(f\"Building and training {model_name}\")\n    print('='*50)\n    \n    # Create and compile model\n    model = model_func()\n    model = model_builder.compile_model(model)\n    \n    print(f\"\\n{model_name} Architecture:\")\n    model.summary()\n    \n    # Train model\n    model, history = model_builder.train_model(\n        model, model_name,\n        X_text_train_balanced, X_num_train_balanced, y_train_balanced,\n        X_text_val, X_num_val, y_val,\n        class_weights, epochs=50\n    )\n    \n    # Plot training history\n    model_builder.plot_training_history(model_name)\n    \n    # Evaluate model\n    results = model_builder.evaluate_model(\n        model, model_name, X_text_test, X_num_test, y_test, class_names\n    )\n    \n    # Plot confusion matrix\n    model_builder.plot_confusion_matrix(results, class_names)\n    \n    all_results.append(results)\n    \n    # Print results\n    print(f\"\\n{model_name} Results:\")\n    print(f\"Accuracy: {results['accuracy']:.4f}\")\n    print(f\"F1-Score (Weighted): {results['f1_weighted']:.4f}\")\n    print(f\"F1-Score (Macro): {results['f1_macro']:.4f}\")\n    print(f\"AUC Score: {results['auc_score']:.4f}\")\n    \n    # Save results\n    with open('data/model_results.pkl', 'wb') as f:\n        pickle.dump(all_results, f)\n    \n\n\n\n#model_configs = [\n#    ('Deep MLP with TF-IDF', lambda: model_builder.create_mlp_model(X_train_mlp.shape[1])),\n#    ('LSTM Model', model_builder.create_lstm_model),\n#    ('BiLSTM with Attention', model_builder.create_robust_model),\n#    ('CNN Model', model_builder.create_cnn_model),\n#    ('Transformer Model', model_builder.create_transformer_model),\n#    ('Hybrid CNN-LSTM', model_builder.create_hybrid_cnn_lstm_model)\n#]\n\n\n# Initialize model builder\nmodel_builder = DeepLearningModels(vocab_size, max_len, num_features, num_classes)\n\n#'Transformer Model', model_builder.create_transformer_model)\n# Define models to train\nmodel_configs = [\n    ('CNN Model', model_builder.create_cnn_model)\n]\n\n# Train and evaluate all models\n\nfor model_name, model_func in model_configs:\n    print(f\"\\n{'='*50}\")\n    print(f\"Building and training {model_name}\")\n    print('='*50)\n    \n    # Create and compile model\n    model = model_func()\n    model = model_builder.compile_model(model)\n    \n    print(f\"\\n{model_name} Architecture:\")\n    model.summary()\n    \n    # Train model\n    model, history = model_builder.train_model(\n        model, model_name,\n        X_text_train_balanced, X_num_train_balanced, y_train_balanced,\n        X_text_val, X_num_val, y_val,\n        class_weights, epochs=50\n    )\n    \n    # Plot training history\n    model_builder.plot_training_history(model_name)\n    \n    # Evaluate model\n    results = model_builder.evaluate_model(\n        model, model_name, X_text_test, X_num_test, y_test, class_names\n    )\n    \n    # Plot confusion matrix\n    model_builder.plot_confusion_matrix(results, class_names)\n    \n    all_results.append(results)\n    \n    # Print results\n    print(f\"\\n{model_name} Results:\")\n    print(f\"Accuracy: {results['accuracy']:.4f}\")\n    print(f\"F1-Score (Weighted): {results['f1_weighted']:.4f}\")\n    print(f\"F1-Score (Macro): {results['f1_macro']:.4f}\")\n    print(f\"AUC Score: {results['auc_score']:.4f}\")\n    \n    # Save results\n    with open('data/model_results.pkl', 'wb') as f:\n        pickle.dump(all_results, f)\n    \n\n\n\n#model_configs = [\n#    ('Deep MLP with TF-IDF', lambda: model_builder.create_mlp_model(X_train_mlp.shape[1])),\n#    ('LSTM Model', model_builder.create_lstm_model),\n#    ('BiLSTM with Attention', model_builder.create_robust_model),\n#    ('CNN Model', model_builder.create_cnn_model),\n#    ('Transformer Model', model_builder.create_transformer_model),\n#    ('Hybrid CNN-LSTM', model_builder.create_hybrid_cnn_lstm_model)\n#]\n\n\n# Initialize model builder\nmodel_builder = DeepLearningModels(vocab_size, max_len, num_features, num_classes)\n\n#'Hybrid CNN-LSTM', model_builder.create_hybrid_cnn_lstm_model)\n# Define models to train\nmodel_configs = [\n    ('Transformer Model', model_builder.create_transformer_model)\n]\n\n# Train and evaluate all models\n\nfor model_name, model_func in model_configs:\n    print(f\"\\n{'='*50}\")\n    print(f\"Building and training {model_name}\")\n    print('='*50)\n    \n    # Create and compile model\n    model = model_func()\n    model = model_builder.compile_model(model)\n    \n    print(f\"\\n{model_name} Architecture:\")\n    model.summary()\n    \n    # Train model\n    model, history = model_builder.train_model(\n        model, model_name,\n        X_text_train_balanced, X_num_train_balanced, y_train_balanced,\n        X_text_val, X_num_val, y_val,\n        class_weights, epochs=50\n    )\n    \n    # Plot training history\n    model_builder.plot_training_history(model_name)\n    \n    # Evaluate model\n    results = model_builder.evaluate_model(\n        model, model_name, X_text_test, X_num_test, y_test, class_names\n    )\n    \n    # Plot confusion matrix\n    model_builder.plot_confusion_matrix(results, class_names)\n    \n    all_results.append(results)\n    \n    # Print results\n    print(f\"\\n{model_name} Results:\")\n    print(f\"Accuracy: {results['accuracy']:.4f}\")\n    print(f\"F1-Score (Weighted): {results['f1_weighted']:.4f}\")\n    print(f\"F1-Score (Macro): {results['f1_macro']:.4f}\")\n    print(f\"AUC Score: {results['auc_score']:.4f}\")\n    \n    # Save results\n    with open('data/model_results.pkl', 'wb') as f:\n        pickle.dump(all_results, f)\n    \n\n\n\n#model_configs = [\n#    ('Deep MLP with TF-IDF', lambda: model_builder.create_mlp_model(X_train_mlp.shape[1])),\n#    ('LSTM Model', model_builder.create_lstm_model),\n#    ('BiLSTM with Attention', model_builder.create_robust_model),\n#    ('CNN Model', model_builder.create_cnn_model),\n#    ('Transformer Model', model_builder.create_transformer_model),\n#    ('Hybrid CNN-LSTM', model_builder.create_hybrid_cnn_lstm_model)\n#]\n\n\n# Initialize model builder\nmodel_builder = DeepLearningModels(vocab_size, max_len, num_features, num_classes)\n\n#'Hybrid CNN-LSTM', model_builder.create_hybrid_cnn_lstm_model)\n# Define models to train\nmodel_configs = [\n    ('Hybrid CNN-LSTM', model_builder.create_hybrid_cnn_lstm_model)\n]\n\n# Train and evaluate all models\n\nfor model_name, model_func in model_configs:\n    print(f\"\\n{'='*50}\")\n    print(f\"Building and training {model_name}\")\n    print('='*50)\n    \n    # Create and compile model\n    model = model_func()\n    model = model_builder.compile_model(model)\n    \n    print(f\"\\n{model_name} Architecture:\")\n    model.summary()\n    \n    # Train model\n    model, history = model_builder.train_model(\n        model, model_name,\n        X_text_train_balanced, X_num_train_balanced, y_train_balanced,\n        X_text_val, X_num_val, y_val,\n        class_weights, epochs=50\n    )\n    \n    # Plot training history\n    model_builder.plot_training_history(model_name)\n    \n    # Evaluate model\n    results = model_builder.evaluate_model(\n        model, model_name, X_text_test, X_num_test, y_test, class_names\n    )\n    \n    # Plot confusion matrix\n    model_builder.plot_confusion_matrix(results, class_names)\n    \n    all_results.append(results)\n    \n    # Print results\n    print(f\"\\n{model_name} Results:\")\n    print(f\"Accuracy: {results['accuracy']:.4f}\")\n    print(f\"F1-Score (Weighted): {results['f1_weighted']:.4f}\")\n    print(f\"F1-Score (Macro): {results['f1_macro']:.4f}\")\n    print(f\"AUC Score: {results['auc_score']:.4f}\")\n    \n    # Save results\n    with open('data/model_results.pkl', 'wb') as f:\n        pickle.dump(all_results, f)\n    \n\n\n\n# Create comparison summary\nsummary_data = []\nfor result in all_results:\n    summary_data.append({\n        'Model': result['model_name'],\n        'Accuracy': result['accuracy'],\n        'F1-Weighted': result['f1_weighted'],\n        'F1-Macro': result['f1_macro'],\n        'AUC Score': result['auc_score']\n    })\n\nsummary_df = pd.DataFrame(summary_data)\nprint(all_results)\n#summary_df = summary_df.sort_values('F1-Weighted', ascending=False)\n\n\n# Create comparison summary\nsummary_data = []\nfor result in all_results:\n    summary_data.append({\n        'Model': result['model_name'],\n        'Accuracy': result['accuracy'],\n        'F1-Weighted': result['f1_weighted'],\n        'F1-Macro': result['f1_macro'],\n        'AUC Score': result['auc_score']\n    })\n\nsummary_df = pd.DataFrame(summary_data)\nsummary_df = summary_df.sort_values('F1-Weighted', ascending=False)\n\nprint(f\"\\n{'='*80}\")\nprint(\"MODEL PERFORMANCE COMPARISON\")\nprint('='*80)\nprint(summary_df.to_string(index=False, float_format='%.4f'))\n\n# Save summary\nsummary_df.to_csv('data/model_comparison_summary.csv', index=False)\n\n# Plot comparison\nplt.figure(figsize=(14, 8))\n\nmetrics = ['Accuracy', 'F1-Weighted', 'F1-Macro', 'AUC Score']\nx = np.arange(len(summary_df))\nwidth = 0.2\n\nfor i, metric in enumerate(metrics):\n    plt.bar(x + i*width, summary_df[metric], width, label=metric, alpha=0.8)\n\nplt.xlabel('Models')\nplt.ylabel('Score')\nplt.title('Deep Learning Models Performance Comparison')\nplt.xticks(x + width*1.5, summary_df['Model'], rotation=45, ha='right')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('charts/model_comparison.png', dpi=300, bbox_inches='tight')\nplt.close()\n\nprint(f\"\\nBest performing model: {summary_df.iloc[0]['Model']}\")\nprint(f\"Best F1-Weighted Score: {summary_df.iloc[0]['F1-Weighted']:.4f}\")\n\n# After training all models, save everything\nresults_path = model_builder.save_all_models(all_results)\nprint(f\"All models and results saved. Results path: {results_path}\")\n    \n    \n\n\n\n", language="python")

with tab_run:
    st.subheader("Available Functions")
    # Collect top-level functions defined by the notebook (not builtins/dunders)
    funcs = []
    for name, obj in env.items():
        if isinstance(obj, types.FunctionType) and not name.startswith("_"):
            # Only include functions that belong to this env
            if obj.__globals__ is env:
                funcs.append((name, obj))

    if not funcs:
        st.info("No callable functions were found in the notebook. "
                "Consider adding helper functions (e.g., `predict(image)`, `classify(x)`, etc.).")
    else:
        funcs.sort(key=lambda x: x[0].lower())
        fn_names = [f[0] for f in funcs]
        choice = st.selectbox("Choose a function to run", fn_names, index=0)
        fn = dict(funcs)[choice]

        sig = inspect.signature(fn)
        st.write("**Signature:**", f"`{{choice}}{{sig}}`")

        st.markdown("#### Provide Arguments")
        inputs = {{}}
        upload_cache = {{}}

        for pname, param in sig.parameters.items():
            ann = str(param.annotation) if param.annotation is not inspect._empty else ""
            default = None if param.default is inspect._empty else param.default

            # Decide input widget type based on name hint
            lname = pname.lower()
            is_image = any(k in lname for k in ["image", "img", "picture", "photo"])
            is_file = is_image or any(k in lname for k in ["file", "bytes", "blob"])

            if is_image:
                uploaded = st.file_uploader(f"Upload image for `{{pname}}`", type=["png","jpg","jpeg","webp","bmp"], key=f"up_{{pname}}")
                if uploaded is not None:
                    data = uploaded.read()
                    upload_cache[pname] = data
                    if PIL_AVAILABLE:
                        try:
                            st.image(Image.open(io.BytesIO(data)), caption=f"{{pname}}")
                        except Exception:
                            st.write("Preview unavailable.")
                else:
                    st.write(f"Or enter a Python expression/JSON for `{{pname}}` below.")
                text_default = "" if default is None else repr(default)
                inputs[pname] = st.text_area(f"`{{pname}}` (JSON or Python expression)", value=text_default, height=80, key=f"txt_{{pname}}")
            elif is_file:
                uploaded = st.file_uploader(f"Upload file for `{{pname}}`", key=f"up_{{pname}}")
                if uploaded is not None:
                    upload_cache[pname] = uploaded.read()
                text_default = "" if default is None else repr(default)
                inputs[pname] = st.text_area(f"`{{pname}}` (JSON or Python expression)", value=text_default, height=80, key=f"txt_{{pname}}")
            else:
                text_default = "" if default is None else json.dumps(default) if isinstance(default, (dict, list, str, int, float, bool, type(None))) else repr(default)
                inputs[pname] = st.text_input(f"`{{pname}}` (JSON or Python expression)", value=text_default, key=f"txt_{{pname}}")

        st.divider()
        if st.button("Run Function"):
            kwargs = {{}}
            # Parse inputs: prefer uploaded bytes; otherwise parse JSON or eval Python
            for pname, val in inputs.items():
                if pname in upload_cache:
                    kwargs[pname] = upload_cache[pname]
                else:
                    text = val.strip()
                    if text == "":
                        continue
                    try:
                        kwargs[pname] = json.loads(text)
                    except Exception:
                        try:
                            kwargs[pname] = eval(text, {{}})
                        except Exception as e:
                            st.error(f"Could not parse value for `{{pname}}`: {{e}}")
                            st.stop()
            try:
                result = fn(**kwargs)
                # Display result intelligently
                if PIL_AVAILABLE and isinstance(result, Image.Image):
                    st.image(result, caption="Function output")
                else:
                    st.write("**Result:**")
                    st.write(result)
            except Exception as e:
                st.error("Error during function execution:")
                st.exception(e)

st.sidebar.markdown("### Tips")
st.sidebar.markdown("- Define clear helper functions in the notebook, e.g., `def predict(image_bytes): ...`")
st.sidebar.markdown("- Name parameters `image`/`img`/`file` to get automatic upload widgets.")
st.sidebar.markdown("- Return PIL Images to get image previews.")
st.sidebar.markdown("- Use JSON for simple arguments or Python expressions (e.g., `[1,2,3]`, `{{'a': 1}}`).")
