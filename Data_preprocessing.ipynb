{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecb17195-a0cf-47ff-82fc-106b16607e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\buw-\n",
      "[nltk_data]     ki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data Preprocessing and Feature Engineering for Customer Satisfaction Prediction\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d878c584-0f3c-4896-8b23-e54f056a4992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and exploring data...\n",
      "Dataset shape: (13594, 11)\n",
      "\n",
      "Target variable distribution:\n",
      "ReviewRating\n",
      "1    7082\n",
      "2     850\n",
      "3     644\n",
      "4    1099\n",
      "5    3919\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class imbalance ratio: 11.00\n",
      "\n",
      "Preprocessing features...\n",
      "Cleaning text data...\n",
      "Extracting text features...\n",
      "\n",
      "Preparing text sequences for deep learning...\n",
      "Text sequences shape: (13594, 100)\n",
      "Vocabulary size: 27318\n",
      "\n",
      "Extracting TF-IDF features...\n",
      "\n",
      "Splitting data into train (70.0%), validation (10.0%), and test (20.0%) sets...\n",
      "Train set size: 9515\n",
      "Validation set size: 1360\n",
      "Test set size: 2719\n",
      "\n",
      "Handling class imbalance using combined strategy...\n",
      "Original distribution: [4957  595  451  769 2743]\n",
      "Balanced distribution: [4957   40 4957 4957 4957]\n",
      "\n",
      "Saved preprocessing artifacts to data/ directory\n",
      "\n",
      "==================================================\n",
      "Data preprocessing completed successfully!\n",
      "==================================================\n",
      "Numerical features shape: (9515, 14)\n",
      "Text features shape: (9515, 100)\n",
      "TF-IDF features shape: (9515, 10014)\n",
      "Balanced training set size: 19868\n",
      "Class weights: {0: np.float64(0.3839015533588864), 1: np.float64(3.1983193277310926), 2: np.float64(4.219512195121951), 3: np.float64(2.4746423927178154), 4: np.float64(0.6937659496901203)}\n"
     ]
    }
   ],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.label_encoders = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.tfidf = None\n",
    "        self.tokenizer = None\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and preprocess text data\"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return ''\n",
    "\n",
    "        # Convert to lowercase\n",
    "        text = str(text).lower()\n",
    "\n",
    "        # Remove URLs, email addresses\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "        # Remove special characters and digits, keep only alphabets and spaces\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Remove stopwords and lemmatize\n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens\n",
    "                  if token not in self.stop_words and len(token) > 2]\n",
    "\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def extract_text_features(self, text):\n",
    "        \"\"\"Extract additional features from text\"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return {\n",
    "                'text_length': 0,\n",
    "                'word_count': 0,\n",
    "                'avg_word_length': 0,\n",
    "                'exclamation_count': 0,\n",
    "                'question_count': 0,\n",
    "                'upper_case_ratio': 0\n",
    "            }\n",
    "\n",
    "        text = str(text)\n",
    "        words = text.split()\n",
    "\n",
    "        return {\n",
    "            'text_length': len(text),\n",
    "            'word_count': len(words),\n",
    "            'avg_word_length': np.mean([len(word) for word in words]) if words else 0,\n",
    "            'exclamation_count': text.count('!'),\n",
    "            'question_count': text.count('?'),\n",
    "            'upper_case_ratio': sum(1 for c in text if c.isupper()) / len(text) if text else 0\n",
    "        }\n",
    "\n",
    "    def load_and_explore_data(self, filepath):\n",
    "        \"\"\"Load data and perform initial exploration\"\"\"\n",
    "        print(\"Loading and exploring data...\")\n",
    "        df = pd.read_csv(filepath)\n",
    "\n",
    "        print(f\"Dataset shape: {df.shape}\")\n",
    "        print(f\"\\nTarget variable distribution:\")\n",
    "        print(df['ReviewRating'].value_counts().sort_index())\n",
    "\n",
    "        # Calculate class imbalance ratio\n",
    "        rating_counts = df['ReviewRating'].value_counts()\n",
    "        imbalance_ratio = rating_counts.max() / rating_counts.min()\n",
    "        print(f\"\\nClass imbalance ratio: {imbalance_ratio:.2f}\")\n",
    "\n",
    "        # Visualize target distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        df['ReviewRating'].value_counts().sort_index().plot(kind='bar')\n",
    "        plt.title('Review Rating Distribution')\n",
    "        plt.xlabel('Rating')\n",
    "        plt.ylabel('Count')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        df['ReviewRating'].value_counts().sort_index().plot(kind='pie', autopct='%1.1f%%')\n",
    "        plt.title('Review Rating Distribution (%)')\n",
    "        plt.ylabel('')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('charts/target_distribution.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        return df\n",
    "\n",
    "    def augment_text_data(self, texts):\n",
    "        augmented = []\n",
    "        for text in texts:\n",
    "            if np.random.rand() > 0.7:  # 30% augmentation chance\n",
    "                words = text.split()\n",
    "                if len(words) > 3:\n",
    "                    try:\n",
    "                        idx = np.random.randint(0, len(words))\n",
    "                        synonyms = wordnet.synsets(words[idx])\n",
    "                        if synonyms:\n",
    "                            new_word = synonyms[0].lemmas()[0].name()\n",
    "                            words[idx] = new_word\n",
    "                    except:\n",
    "                        pass\n",
    "                augmented.append(' '.join(words))\n",
    "            else:\n",
    "                augmented.append(text)\n",
    "        return augmented\n",
    "\n",
    "    def preprocess_features(self, df):\n",
    "        \"\"\"Preprocess all features\"\"\"\n",
    "        print(\"\\nPreprocessing features...\")\n",
    "\n",
    "        # Clean text features\n",
    "        print(\"Cleaning text data...\")\n",
    "        df['ReviewText_clean'] = df['ReviewText'].apply(self.clean_text)\n",
    "        df['ReviewTitle_clean'] = df['ReviewTitle'].apply(self.clean_text)\n",
    "\n",
    "        df['ReviewText_clean'] = self.augment_text_data(df['ReviewText_clean'])\n",
    "        df['ReviewTitle_clean'] = self.augment_text_data(df['ReviewTitle_clean'])\n",
    "\n",
    "        # Combine text features\n",
    "        df['combined_text'] = df['ReviewText_clean'] + ' ' + df['ReviewTitle_clean']\n",
    "\n",
    "        # Extract text features\n",
    "        print(\"Extracting text features...\")\n",
    "        text_features = df['ReviewText'].apply(self.extract_text_features)\n",
    "        text_features_df = pd.DataFrame(text_features.tolist())\n",
    "\n",
    "        title_features = df['ReviewTitle'].apply(self.extract_text_features)\n",
    "        title_features_df = pd.DataFrame(title_features.tolist())\n",
    "        title_features_df.columns = ['title_' + col for col in title_features_df.columns]\n",
    "\n",
    "        # Combine all features\n",
    "        df = pd.concat([df, text_features_df, title_features_df], axis=1)\n",
    "\n",
    "        # Encode categorical features\n",
    "        categorical_features = ['UserCountry']\n",
    "        for feature in categorical_features:\n",
    "            if feature in df.columns:\n",
    "                le = LabelEncoder()\n",
    "                df[f'{feature}_encoded'] = le.fit_transform(df[feature].astype(str))\n",
    "                self.label_encoders[feature] = le\n",
    "\n",
    "        # Select final features for modeling\n",
    "        feature_columns = [\n",
    "            'ReviewCount', 'UserCountry_encoded',\n",
    "            'text_length', 'word_count', 'avg_word_length',\n",
    "            'exclamation_count', 'question_count', 'upper_case_ratio',\n",
    "            'title_text_length', 'title_word_count', 'title_avg_word_length',\n",
    "            'title_exclamation_count', 'title_question_count', 'title_upper_case_ratio'\n",
    "        ]\n",
    "\n",
    "        # Handle missing values\n",
    "        for col in feature_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(0)\n",
    "\n",
    "        return df, feature_columns\n",
    "\n",
    "    def create_balanced_dataset(self, X, y, strategy='combined'):\n",
    "        \"\"\"Handle class imbalance using various techniques\"\"\"\n",
    "        print(f\"\\nHandling class imbalance using {strategy} strategy...\")\n",
    "\n",
    "        if strategy == 'smote':\n",
    "            # Use SMOTE for oversampling\n",
    "            smote = SMOTE(random_state=42, k_neighbors=2)  # Reduced k_neighbors due to small dataset\n",
    "            X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "\n",
    "        elif strategy == 'undersampling':\n",
    "            # Use random undersampling\n",
    "            undersampler = RandomUnderSampler(random_state=42)\n",
    "            X_balanced, y_balanced = undersampler.fit_resample(X, y)\n",
    "\n",
    "        elif strategy == 'combined':\n",
    "            # Combined approach: first oversample minority classes, then undersample majority\n",
    "            # Step 1: Oversample very minority classes\n",
    "            smote = SMOTE(random_state=42, k_neighbors=1)\n",
    "            X_temp, y_temp = smote.fit_resample(X, y)\n",
    "\n",
    "            # Step 2: Undersample majority class\n",
    "            undersampler = RandomUnderSampler(random_state=42,\n",
    "                                              sampling_strategy={1: 40})  # Reduce class 1 to 40 samples\n",
    "            X_balanced, y_balanced = undersampler.fit_resample(X_temp, y_temp)\n",
    "\n",
    "        else:\n",
    "            X_balanced, y_balanced = X, y\n",
    "\n",
    "        print(f\"Original distribution: {np.bincount(y)}\")\n",
    "        print(f\"Balanced distribution: {np.bincount(y_balanced)}\")\n",
    "\n",
    "        return X_balanced, y_balanced\n",
    "\n",
    "    def prepare_text_sequences(self, df, max_features=5000, max_len=100):\n",
    "        \"\"\"Prepare text data for deep learning models\"\"\"\n",
    "        from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "        from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "        print(\"\\nPreparing text sequences for deep learning...\")\n",
    "\n",
    "        # Initialize tokenizer\n",
    "        tokenizer = Tokenizer(num_words=max_features, oov_token='<OOV>')\n",
    "        tokenizer.fit_on_texts(df['combined_text'])\n",
    "\n",
    "        # Convert texts to sequences\n",
    "        sequences = tokenizer.texts_to_sequences(df['combined_text'])\n",
    "\n",
    "        # Pad sequences\n",
    "        X_text = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "        print(f\"Text sequences shape: {X_text.shape}\")\n",
    "        print(f\"Vocabulary size: {len(tokenizer.word_index)}\")\n",
    "\n",
    "        return X_text, tokenizer\n",
    "\n",
    "    def split_data(self, X_numerical, X_text, y, test_size=0.2, val_size=0.1):\n",
    "        \"\"\"Split data into train, validation, and test sets\"\"\"\n",
    "        print(\n",
    "            f\"\\nSplitting data into train ({1 - test_size - val_size:.1%}), validation ({val_size:.1%}), and test ({test_size:.1%}) sets...\")\n",
    "\n",
    "        # First split: separate test set\n",
    "        X_num_temp, X_num_test, X_text_temp, X_text_test, y_temp, y_test = train_test_split(\n",
    "            X_numerical, X_text, y, test_size=test_size, random_state=42, stratify=y\n",
    "        )\n",
    "\n",
    "        # Second split: separate train and validation from remaining data\n",
    "        val_size_adjusted = val_size / (1 - test_size)  # Adjust validation size\n",
    "        X_num_train, X_num_val, X_text_train, X_text_val, y_train, y_val = train_test_split(\n",
    "            X_num_temp, X_text_temp, y_temp, test_size=val_size_adjusted, random_state=42, stratify=y_temp\n",
    "        )\n",
    "\n",
    "        print(f\"Train set size: {len(X_num_train)}\")\n",
    "        print(f\"Validation set size: {len(X_num_val)}\")\n",
    "        print(f\"Test set size: {len(X_num_test)}\")\n",
    "\n",
    "        # Scale numerical features\n",
    "        X_num_train_scaled = self.scaler.fit_transform(X_num_train)\n",
    "        X_num_val_scaled = self.scaler.transform(X_num_val)\n",
    "        X_num_test_scaled = self.scaler.transform(X_num_test)\n",
    "\n",
    "        return (X_num_train_scaled, X_num_val_scaled, X_num_test_scaled,\n",
    "                X_text_train, X_text_val, X_text_test,\n",
    "                y_train, y_val, y_test)\n",
    "\n",
    "\n",
    "    def extract_tfidf_features(self, df, max_features=5000):\n",
    "        \"\"\"Extract TF-IDF features from combined text\"\"\"\n",
    "        print(\"\\nExtracting TF-IDF features...\")\n",
    "\n",
    "        # Combine text features\n",
    "        df['combined_text'] = df['ReviewText_clean'] + ' ' + df['ReviewTitle_clean']\n",
    "\n",
    "        # Initialize TF-IDF vectorizer\n",
    "        self.tfidf = TfidfVectorizer(\n",
    "            max_features=max_features * 2,\n",
    "            ngram_range=(1, 3),  # Include unigrams, bigrams and trigrams\n",
    "            min_df=5,  # Ignore rare terms\n",
    "            max_df=0.75,  # Ignore overly common terms\n",
    "            sublinear_tf=True,  # Use log scaling\n",
    "            stop_words='english'\n",
    "        )\n",
    "\n",
    "        # Fit and transform\n",
    "        tfidf_features = self.tfidf.fit_transform(df['combined_text'])\n",
    "\n",
    "        # Convert to dense array (if memory allows)\n",
    "        return tfidf_features.toarray()\n",
    "\n",
    "    def prepare_mlp_data(self, df, test_size=0.2, val_size=0.1):\n",
    "        \"\"\"Prepare data for MLP model with TF-IDF features\"\"\"\n",
    "        # Extract TF-IDF features\n",
    "        X_text = self.extract_tfidf_features(df)\n",
    "\n",
    "        # Get numerical features\n",
    "        numerical_features = [\n",
    "            'ReviewCount', 'UserCountry_encoded',\n",
    "            'text_length', 'word_count', 'avg_word_length',\n",
    "            'exclamation_count', 'question_count', 'upper_case_ratio',\n",
    "            'title_text_length', 'title_word_count', 'title_avg_word_length',\n",
    "            'title_exclamation_count', 'title_question_count', 'title_upper_case_ratio'\n",
    "        ]\n",
    "\n",
    "        X_num = df[numerical_features].values\n",
    "\n",
    "        # Combine features\n",
    "        X = np.concatenate([X_num, X_text], axis=1)\n",
    "        y = df['ReviewRating'].values - 1  # Convert to 0-4\n",
    "\n",
    "        # Split data\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=42, stratify=y\n",
    "        )\n",
    "\n",
    "        val_size_adjusted = val_size / (1 - test_size)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=val_size_adjusted, random_state=42, stratify=y_temp\n",
    "        )\n",
    "\n",
    "        # Scale numerical features only (TF-IDF is already normalized)\n",
    "        num_features = X_num.shape[1]\n",
    "        scaler = StandardScaler()\n",
    "        X_train[:, :num_features] = scaler.fit_transform(X_train[:, :num_features])\n",
    "        X_val[:, :num_features] = scaler.transform(X_val[:, :num_features])\n",
    "        X_test[:, :num_features] = scaler.transform(X_test[:, :num_features])\n",
    "\n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "    def save_preprocessed_data(self, output_dir='data'):\n",
    "        \"\"\"Save all preprocessing artifacts to disk\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        artifacts = {\n",
    "            'label_encoders.pkl': self.label_encoders,\n",
    "            'scaler.pkl': self.scaler,\n",
    "            'tokenizer.pkl': self.tokenizer,\n",
    "            'tfidf.pkl': self.tfidf\n",
    "        }\n",
    "\n",
    "        for filename, obj in artifacts.items():\n",
    "            if obj is not None:  # Only save if the object exists\n",
    "                with open(os.path.join(output_dir, filename), 'wb') as f:\n",
    "                    pickle.dump(obj, f)\n",
    "\n",
    "        print(f\"\\nSaved preprocessing artifacts to {output_dir}/ directory\")\n",
    "\n",
    "def main():\n",
    "    # Create output directories\n",
    "    os.makedirs('charts', exist_ok=True)\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "\n",
    "    # Initialize preprocessor\n",
    "    preprocessor = DataPreprocessor()\n",
    "\n",
    "    # Load and explore data\n",
    "    df = preprocessor.load_and_explore_data('data/temu_reviews_cleaned.csv')\n",
    "\n",
    "    # Preprocess features\n",
    "    df_processed, feature_columns = preprocessor.preprocess_features(df)\n",
    "\n",
    "    # Prepare numerical features\n",
    "    X_numerical = df_processed[feature_columns].values\n",
    "\n",
    "    # Prepare text sequences\n",
    "    X_text, tokenizer = preprocessor.prepare_text_sequences(df_processed)\n",
    "\n",
    "    # Prepare TF-IDF features for MLP\n",
    "    X_train_mlp, X_val_mlp, X_test_mlp, y_train_mlp, y_val_mlp, y_test_mlp = \\\n",
    "        preprocessor.prepare_mlp_data(df_processed)\n",
    "\n",
    "    # Prepare target variable\n",
    "    y = df_processed['ReviewRating'].values - 1  # Convert to 0-4 for neural networks\n",
    "\n",
    "    # Split data\n",
    "    (X_num_train, X_num_val, X_num_test,\n",
    "     X_text_train, X_text_val, X_text_test,\n",
    "     y_train, y_val, y_test) = preprocessor.split_data(X_numerical, X_text, y)\n",
    "\n",
    "    # Handle class imbalance for training data only\n",
    "    # Combine numerical and text features for balancing\n",
    "    X_train_combined = np.concatenate([X_num_train, X_text_train], axis=1)\n",
    "    X_train_balanced, y_train_balanced = preprocessor.create_balanced_dataset(\n",
    "        X_train_combined, y_train, strategy='combined'\n",
    "    )\n",
    "\n",
    "    # Split back into numerical and text features\n",
    "    num_features = X_num_train.shape[1]\n",
    "    X_num_train_balanced = X_train_balanced[:, :num_features]\n",
    "    X_text_train_balanced = X_train_balanced[:, num_features:].astype(int)\n",
    "\n",
    "    # Calculate class weights for models\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "    # Save preprocessed data\n",
    "    np.savez('data/preprocessed_data.npz',\n",
    "             X_num_train=X_num_train, X_num_val=X_num_val, X_num_test=X_num_test,\n",
    "             X_text_train=X_text_train, X_text_val=X_text_val, X_text_test=X_text_test,\n",
    "             X_num_train_balanced=X_num_train_balanced,\n",
    "             X_text_train_balanced=X_text_train_balanced,\n",
    "             y_train=y_train, y_val=y_val, y_test=y_test,\n",
    "             y_train_balanced=y_train_balanced,\n",
    "             X_train_mlp=X_train_mlp, X_val_mlp=X_val_mlp, X_test_mlp=X_test_mlp,\n",
    "             y_train_mlp=y_train_mlp, y_val_mlp=y_val_mlp, y_test_mlp=y_test_mlp)\n",
    "\n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'feature_columns': feature_columns,\n",
    "        'vocab_size': len(tokenizer.word_index) + 1,\n",
    "        'max_sequence_length': X_text.shape[1],\n",
    "        'num_classes': len(np.unique(y)),\n",
    "        'class_weights': class_weight_dict,\n",
    "        'class_names': ['Rating 1', 'Rating 2', 'Rating 3', 'Rating 4', 'Rating 5'],\n",
    "        'tfidf_features': preprocessor.tfidf.get_feature_names_out().shape[0] if preprocessor.tfidf else 0\n",
    "    }\n",
    "\n",
    "    with open('data/metadata.pkl', 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "\n",
    "    with open('data/tokenizer.pkl', 'wb') as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "\n",
    "    # Save all preprocessing artifacts\n",
    "    preprocessor.save_preprocessed_data()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Data preprocessing completed successfully!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Numerical features shape: {X_num_train.shape}\")\n",
    "    print(f\"Text features shape: {X_text_train.shape}\")\n",
    "    print(f\"TF-IDF features shape: {X_train_mlp.shape}\")\n",
    "    print(f\"Balanced training set size: {len(X_num_train_balanced)}\")\n",
    "    print(f\"Class weights: {class_weight_dict}\")\n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0137338-b4fc-465e-bf86-5023538bd157",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
